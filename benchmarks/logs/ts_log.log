2022-09-29T22:00:20,584 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:00:20,584 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:00:20,719 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:00:20,719 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:00:20,730 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:00:20,730 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:00:20,763 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:00:20,763 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:00:20,829 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:00:20,829 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:00:21,120 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,121 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:163.31096649169922|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:57.954715728759766|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:26.2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:3384.48828125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,123 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3423.6328125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,123 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.7|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:22,945 [INFO ] pool-2-thread-1 ACCESS_LOG - /127.0.0.1:37808 "GET /ping HTTP/1.1" 200 7
2022-09-29T22:00:22,946 [INFO ] pool-2-thread-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:34,200 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:00:34,200 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:00:34,202 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:00:34,202 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:00:34,202 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:00:34,202 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:00:34,204 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:00:34,204 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:00:34,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:00:34,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:00:34,831 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-09-29T22:00:34,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - [PID]19736
2022-09-29T22:00:34,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Torch worker started.
2022-09-29T22:00:34,833 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Python runtime: 3.10.6
2022-09-29T22:00:34,833 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:00:34,833 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:00:34,837 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:00:34,837 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:00:34,843 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-09-29T22:00:34,845 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469034845
2022-09-29T22:00:34,845 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469034845
2022-09-29T22:00:34,854 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - model_name: benchmark, batchSize: 1
2022-09-29T22:00:35,715 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 861
2022-09-29T22:00:35,715 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 861
2022-09-29T22:00:35,716 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:00:35,716 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:00:35,716 [INFO ] W-9000-benchmark_1.0 TS_METRICS - W-9000-benchmark_1.0.ms:1507|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,716 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,716 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:35646 "POST /models?model_name=benchmark&url=https%3A%2F%2Ftorchserve.pytorch.org%2Fmar_files%2Fresnet-18.mar&batch_delay=200&batch_size=1&initial_workers=1&synchronous=true HTTP/1.1" 200 12755
2022-09-29T22:00:35,716 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,745 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035745
2022-09-29T22:00:35,745 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035745
2022-09-29T22:00:35,748 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:120.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:38bc511b-3e14-4428-9c20-3925855817dc,timestamp:1664469035
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58898 "POST /predictions/benchmark HTTP/1.0" 200 129
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:38bc511b-3e14-4428-9c20-3925855817dc,timestamp:1664469035
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,870 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 141253, Backend time ns: 125001630
2022-09-29T22:00:35,870 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 141253, Backend time ns: 125001630
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035880
2022-09-29T22:00:35,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035880
2022-09-29T22:00:35,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:85.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:59ce105d-57ce-4f3b-af08-f632844ddc17,timestamp:1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:85.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:59ce105d-57ce-4f3b-af08-f632844ddc17,timestamp:1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58898 "POST /predictions/benchmark HTTP/1.0" 200 91
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,968 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84500, Backend time ns: 88006739
2022-09-29T22:00:35,968 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84500, Backend time ns: 88006739
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035968
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035968
2022-09-29T22:00:35,969 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:36,020 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,020 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0eef0230-4bb9-4136-ba0c-3ad291d674df,timestamp:1664469036
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58910 "POST /predictions/benchmark HTTP/1.0" 200 127
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0eef0230-4bb9-4136-ba0c-3ad291d674df,timestamp:1664469036
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,021 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 73904132, Backend time ns: 53019771
2022-09-29T22:00:36,021 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 73904132, Backend time ns: 53019771
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:73|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036022
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036022
2022-09-29T22:00:36,023 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:49465d44-151b-4699-83c4-6ba33754e400,timestamp:1664469036
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:49465d44-151b-4699-83c4-6ba33754e400,timestamp:1664469036
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58916 "POST /predictions/benchmark HTTP/1.0" 200 171
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 114451859, Backend time ns: 56477272
2022-09-29T22:00:36,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 114451859, Backend time ns: 56477272
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:114|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036079
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036079
2022-09-29T22:00:36,080 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.0|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dba103ce-a070-4ddf-aa32-b937910945e7,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dba103ce-a070-4ddf-aa32-b937910945e7,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58946 "POST /predictions/benchmark HTTP/1.0" 200 223
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,132 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169202035, Backend time ns: 53081067
2022-09-29T22:00:36,132 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169202035, Backend time ns: 53081067
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:169|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,133 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036133
2022-09-29T22:00:36,133 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036133
2022-09-29T22:00:36,134 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b342e73e-fd5f-4da9-97ed-7abc2ca5163a,timestamp:1664469036
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58926 "POST /predictions/benchmark HTTP/1.0" 200 275
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b342e73e-fd5f-4da9-97ed-7abc2ca5163a,timestamp:1664469036
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,187 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 221050540, Backend time ns: 54221582
2022-09-29T22:00:36,187 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 221050540, Backend time ns: 54221582
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:221|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036187
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036187
2022-09-29T22:00:36,188 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58956 "POST /predictions/benchmark HTTP/1.0" 200 318
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,241 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264579777, Backend time ns: 53559648
2022-09-29T22:00:36,241 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264579777, Backend time ns: 53559648
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:264|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036241
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036241
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e66e1183-e7de-4744-b567-ba5d2232f923,timestamp:1664469036
2022-09-29T22:00:36,242 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e66e1183-e7de-4744-b567-ba5d2232f923,timestamp:1664469036
2022-09-29T22:00:36,243 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bb8df1c3-be68-4967-8e7b-3c74bff2652b,timestamp:1664469036
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bb8df1c3-be68-4967-8e7b-3c74bff2652b,timestamp:1664469036
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58934 "POST /predictions/benchmark HTTP/1.0" 200 370
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,296 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 315628077, Backend time ns: 54122336
2022-09-29T22:00:36,296 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 315628077, Backend time ns: 54122336
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:315|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036296
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036296
2022-09-29T22:00:36,297 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:68.71|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35bac8d2-73b3-4a8b-9b63-8c9c07ce3bff,timestamp:1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:68.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35bac8d2-73b3-4a8b-9b63-8c9c07ce3bff,timestamp:1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58972 "POST /predictions/benchmark HTTP/1.0" 200 439
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,367 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 368358097, Backend time ns: 70617620
2022-09-29T22:00:36,367 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 368358097, Backend time ns: 70617620
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:368|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036367
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036367
2022-09-29T22:00:36,368 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:73d44da5-bf66-4d8a-ba04-d8e04f209fdb,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58918 "POST /predictions/benchmark HTTP/1.0" 200 489
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:73d44da5-bf66-4d8a-ba04-d8e04f209fdb,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,419 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 432300999, Backend time ns: 51937954
2022-09-29T22:00:36,419 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 432300999, Backend time ns: 51937954
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:432|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,431 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036431
2022-09-29T22:00:36,431 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036431
2022-09-29T22:00:36,432 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:80.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d5ec54f6-c5c5-4451-83b8-03653a1fd5d9,timestamp:1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 83
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:80.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d5ec54f6-c5c5-4451-83b8-03653a1fd5d9,timestamp:1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,514 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 64023, Backend time ns: 83015632
2022-09-29T22:00:36,514 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 64023, Backend time ns: 83015632
2022-09-29T22:00:36,514 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,514 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036519
2022-09-29T22:00:36,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036519
2022-09-29T22:00:36,520 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,595 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9fe3ad91-d7ef-4c77-b437-8313fa84d5ee,timestamp:1664469036
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9fe3ad91-d7ef-4c77-b437-8313fa84d5ee,timestamp:1664469036
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 78
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,596 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 223476, Backend time ns: 77292470
2022-09-29T22:00:36,596 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 223476, Backend time ns: 77292470
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036597
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036597
2022-09-29T22:00:36,598 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f48a73f2-1d05-4b39-b4be-85d55f2957dd,timestamp:1664469036
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f48a73f2-1d05-4b39-b4be-85d55f2957dd,timestamp:1664469036
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 130
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,654 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 72249218, Backend time ns: 57296292
2022-09-29T22:00:36,654 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 72249218, Backend time ns: 57296292
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:72|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,655 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036655
2022-09-29T22:00:36,655 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036655
2022-09-29T22:00:36,656 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.46|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:894b13bc-3cb9-4fb3-9790-461b9615bdfb,timestamp:1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:894b13bc-3cb9-4fb3-9790-461b9615bdfb,timestamp:1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 181
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,710 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 125724380, Backend time ns: 54944451
2022-09-29T22:00:36,710 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 125724380, Backend time ns: 54944451
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036710
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036710
2022-09-29T22:00:36,711 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9e8859b-4460-4939-9bc0-6e096d9dae33,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 229
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9e8859b-4460-4939-9bc0-6e096d9dae33,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,764 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 174683287, Backend time ns: 54086360
2022-09-29T22:00:36,764 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 174683287, Backend time ns: 54086360
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:174|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,765 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036764
2022-09-29T22:00:36,765 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036764
2022-09-29T22:00:36,766 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d504bdbe-c8c9-419b-8471-bb1437cf68cf,timestamp:1664469036
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 281
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.85|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d504bdbe-c8c9-419b-8471-bb1437cf68cf,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,818 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 228091381, Backend time ns: 53238757
2022-09-29T22:00:36,818 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 228091381, Backend time ns: 53238757
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:228|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036818
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036818
2022-09-29T22:00:36,820 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:89bbcdd7-9af0-4234-9254-740845e40e2e,timestamp:1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:89bbcdd7-9af0-4234-9254-740845e40e2e,timestamp:1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 359
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,897 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 281095630, Backend time ns: 78310320
2022-09-29T22:00:36,897 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 281095630, Backend time ns: 78310320
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:281|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036897
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036897
2022-09-29T22:00:36,898 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:56f6fd8c-d9ce-4ad4-b439-78dea3912aa1,timestamp:1664469036
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:56f6fd8c-d9ce-4ad4-b439-78dea3912aa1,timestamp:1664469036
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 413
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,951 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 358505946, Backend time ns: 54077998
2022-09-29T22:00:36,951 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 358505946, Backend time ns: 54077998
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:358|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,952 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036951
2022-09-29T22:00:36,952 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036951
2022-09-29T22:00:36,953 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:504ad887-77cc-45c5-a7b2-726e813ebcdd,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:504ad887-77cc-45c5-a7b2-726e813ebcdd,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 464
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 408745506, Backend time ns: 54687967
2022-09-29T22:00:37,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 408745506, Backend time ns: 54687967
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:408|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,007 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037007
2022-09-29T22:00:37,007 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037007
2022-09-29T22:00:37,008 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:facd1925-bb98-4144-ae11-0a77ca6e3690,timestamp:1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 508
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:facd1925-bb98-4144-ae11-0a77ca6e3690,timestamp:1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,061 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 454254255, Backend time ns: 53969238
2022-09-29T22:00:37,061 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 454254255, Backend time ns: 53969238
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:454|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037061
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037061
2022-09-29T22:00:37,062 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:981b1548-4cfc-4e05-b6f8-5568b846f124,timestamp:1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:981b1548-4cfc-4e05-b6f8-5568b846f124,timestamp:1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 566
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,124 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 503681605, Backend time ns: 63126022
2022-09-29T22:00:37,124 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 503681605, Backend time ns: 63126022
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:503|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037124
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037124
2022-09-29T22:00:37,125 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a41be723-c409-4882-999d-a0ca70f6bbbc,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.83|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a41be723-c409-4882-999d-a0ca70f6bbbc,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 580
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,178 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 525506829, Backend time ns: 53978552
2022-09-29T22:00:37,178 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 525506829, Backend time ns: 53978552
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:525|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,179 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037179
2022-09-29T22:00:37,179 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037179
2022-09-29T22:00:37,181 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.98|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:66d0b088-1d74-44e5-88ee-5532cfc3c852,timestamp:1664469037
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:66d0b088-1d74-44e5-88ee-5532cfc3c852,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 578
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,234 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 521708272, Backend time ns: 55483905
2022-09-29T22:00:37,234 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 521708272, Backend time ns: 55483905
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:521|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037234
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037234
2022-09-29T22:00:37,235 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bd362d7a-baa6-4ae3-8e4d-9598c5ace953,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bd362d7a-baa6-4ae3-8e4d-9598c5ace953,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,291 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522344641, Backend time ns: 56833551
2022-09-29T22:00:37,291 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522344641, Backend time ns: 56833551
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037291
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037291
2022-09-29T22:00:37,292 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.39|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:75536641-2dbc-4ae3-8c41-8e2eaffe0a90,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:75536641-2dbc-4ae3-8c41-8e2eaffe0a90,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 580
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,346 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 524611094, Backend time ns: 54601801
2022-09-29T22:00:37,346 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 524611094, Backend time ns: 54601801
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:524|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037346
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037346
2022-09-29T22:00:37,347 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f843cdda-5b9b-463b-ae40-fe25d6166182,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f843cdda-5b9b-463b-ae40-fe25d6166182,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 581
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,400 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 526769833, Backend time ns: 53934595
2022-09-29T22:00:37,400 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 526769833, Backend time ns: 53934595
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:526|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037400
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037400
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:293c989b-dfc2-4906-be67-393544c26a95,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:293c989b-dfc2-4906-be67-393544c26a95,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,454 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501569208, Backend time ns: 53748109
2022-09-29T22:00:37,454 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501569208, Backend time ns: 53748109
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037454
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037454
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:91bb368f-6a57-48b9-b5be-782e575e01bb,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:91bb368f-6a57-48b9-b5be-782e575e01bb,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,508 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501662253, Backend time ns: 53280897
2022-09-29T22:00:37,508 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501662253, Backend time ns: 53280897
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037508
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037508
2022-09-29T22:00:37,509 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:395e08db-836c-45fc-b5b1-e7b6f785b66b,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:395e08db-836c-45fc-b5b1-e7b6f785b66b,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 558
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,566 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 499906854, Backend time ns: 58355639
2022-09-29T22:00:37,566 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 499906854, Backend time ns: 58355639
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:499|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037567
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037567
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:be01102e-f31e-4238-ac65-ef71a76c8e34,timestamp:1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:be01102e-f31e-4238-ac65-ef71a76c8e34,timestamp:1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 558
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 486132656, Backend time ns: 53283862
2022-09-29T22:00:37,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 486132656, Backend time ns: 53283862
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:486|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037620
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037620
2022-09-29T22:00:37,621 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7e926a4f-ccec-4e28-9c83-a2ad342d54ea,timestamp:1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7e926a4f-ccec-4e28-9c83-a2ad342d54ea,timestamp:1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 548
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494579267, Backend time ns: 53412777
2022-09-29T22:00:37,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494579267, Backend time ns: 53412777
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:494|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037674
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037674
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7f7b6e6-bfe4-45c1-8dac-2a34de999cc3,timestamp:1664469037
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 547
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 493488637, Backend time ns: 53700670
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7f7b6e6-bfe4-45c1-8dac-2a34de999cc3,timestamp:1664469037
2022-09-29T22:00:37,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 493488637, Backend time ns: 53700670
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:493|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037728
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037728
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:58.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7d2d39ce-623b-461f-adf4-da97ebcda3e6,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 552
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,787 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492129145, Backend time ns: 59468059
2022-09-29T22:00:37,787 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492129145, Backend time ns: 59468059
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:58.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7d2d39ce-623b-461f-adf4-da97ebcda3e6,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037787
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037787
2022-09-29T22:00:37,788 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:cbdc37e0-5a7a-49aa-81eb-49c58000a4df,timestamp:1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:cbdc37e0-5a7a-49aa-81eb-49c58000a4df,timestamp:1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494642842, Backend time ns: 77327067
2022-09-29T22:00:37,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494642842, Backend time ns: 77327067
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:494|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037865
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037865
2022-09-29T22:00:37,866 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,918 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,918 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0b591f68-952e-4755-8d2f-ae24956f6e36,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.96|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0b591f68-952e-4755-8d2f-ae24956f6e36,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,919 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517421167, Backend time ns: 53964487
2022-09-29T22:00:37,919 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517421167, Backend time ns: 53964487
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037919
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037919
2022-09-29T22:00:37,920 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8f35ea11-97bc-4c93-a36a-8730b28d58d1,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.65|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8f35ea11-97bc-4c93-a36a-8730b28d58d1,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,973 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517562220, Backend time ns: 53476619
2022-09-29T22:00:37,973 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517562220, Backend time ns: 53476619
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037973
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037973
2022-09-29T22:00:37,974 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a9c43b7c-1c46-4166-a402-319ecc7ab293,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a9c43b7c-1c46-4166-a402-319ecc7ab293,timestamp:1664469038
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 573
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517274971, Backend time ns: 55092960
2022-09-29T22:00:38,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517274971, Backend time ns: 55092960
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038028
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038028
2022-09-29T22:00:38,029 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6a391172-5f93-45eb-bcee-71ab299416bd,timestamp:1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6a391172-5f93-45eb-bcee-71ab299416bd,timestamp:1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 574
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,084 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509694903, Backend time ns: 55537091
2022-09-29T22:00:38,084 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509694903, Backend time ns: 55537091
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:509|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038084
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038084
2022-09-29T22:00:38,085 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:57.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e753d5a-a820-4f3a-8551-f0b6ea92f20b,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 575
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:57.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e753d5a-a820-4f3a-8551-f0b6ea92f20b,timestamp:1664469038
2022-09-29T22:00:38,144 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 516266339, Backend time ns: 59647722
2022-09-29T22:00:38,144 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 516266339, Backend time ns: 59647722
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:516|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038144
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038144
2022-09-29T22:00:38,145 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d01f0f02-eb3e-4447-868e-e2f8676abf7c,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 576
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d01f0f02-eb3e-4447-868e-e2f8676abf7c,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522518552, Backend time ns: 53373652
2022-09-29T22:00:38,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522518552, Backend time ns: 53373652
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038197
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038197
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:57.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:589c2ddb-3bcf-4f84-bedf-919d4ab572da,timestamp:1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:57.85|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:589c2ddb-3bcf-4f84-bedf-919d4ab572da,timestamp:1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 582
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,257 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522836378, Backend time ns: 59866002
2022-09-29T22:00:38,257 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522836378, Backend time ns: 59866002
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038258
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038258
2022-09-29T22:00:38,259 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ceffefe8-8c74-44eb-bbdc-800f964d88ad,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ceffefe8-8c74-44eb-bbdc-800f964d88ad,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 583
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,311 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529044912, Backend time ns: 53675043
2022-09-29T22:00:38,311 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529044912, Backend time ns: 53675043
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:529|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038312
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038312
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:58.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc6f8917-efe0-4095-837c-f11cdab9547a,timestamp:1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:58.46|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc6f8917-efe0-4095-837c-f11cdab9547a,timestamp:1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 583
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,372 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522979626, Backend time ns: 59939674
2022-09-29T22:00:38,372 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522979626, Backend time ns: 59939674
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038372
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038372
2022-09-29T22:00:38,373 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:af75d3c3-3a15-407b-944d-b8e020416f22,timestamp:1664469038
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:af75d3c3-3a15-407b-944d-b8e020416f22,timestamp:1664469038
2022-09-29T22:00:38,426 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505609889, Backend time ns: 54146609
2022-09-29T22:00:38,426 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505609889, Backend time ns: 54146609
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:505|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038426
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038426
2022-09-29T22:00:38,427 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:71c9af56-ac11-415a-ab3d-c8ac94cef4ab,timestamp:1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:71c9af56-ac11-415a-ab3d-c8ac94cef4ab,timestamp:1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 561
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,482 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506065267, Backend time ns: 55560829
2022-09-29T22:00:38,482 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506065267, Backend time ns: 55560829
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:506|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038482
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038482
2022-09-29T22:00:38,483 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4661708c-6c63-4c2c-a284-808f8f6cf11a,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4661708c-6c63-4c2c-a284-808f8f6cf11a,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 562
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,536 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508054233, Backend time ns: 53993017
2022-09-29T22:00:38,536 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508054233, Backend time ns: 53993017
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:508|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038536
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038536
2022-09-29T22:00:38,537 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.0|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e9711fa-9ccf-46a9-af8b-a1edb31e1d7d,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 561
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e9711fa-9ccf-46a9-af8b-a1edb31e1d7d,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,590 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506675718, Backend time ns: 53885026
2022-09-29T22:00:38,590 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506675718, Backend time ns: 53885026
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:506|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038590
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038590
2022-09-29T22:00:38,591 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5d8aa477-e8fa-4ae1-b958-9ab69b0cd302,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5d8aa477-e8fa-4ae1-b958-9ab69b0cd302,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,645 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504826551, Backend time ns: 55110865
2022-09-29T22:00:38,645 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504826551, Backend time ns: 55110865
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:504|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,646 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038646
2022-09-29T22:00:38,646 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038646
2022-09-29T22:00:38,647 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0f8ce9a-f591-4348-99ed-47858889b439,timestamp:1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 568
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0f8ce9a-f591-4348-99ed-47858889b439,timestamp:1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,713 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500741620, Backend time ns: 67934413
2022-09-29T22:00:38,713 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500741620, Backend time ns: 67934413
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:500|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038714
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038714
2022-09-29T22:00:38,715 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20eb7948-10ed-4a86-8704-4563c22f3637,timestamp:1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20eb7948-10ed-4a86-8704-4563c22f3637,timestamp:1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 569
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,768 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 515257115, Backend time ns: 53979481
2022-09-29T22:00:38,768 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 515257115, Backend time ns: 53979481
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:515|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038768
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038768
2022-09-29T22:00:38,769 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7df7177-8708-435b-887a-465b3d3bd85c,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 564
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7df7177-8708-435b-887a-465b3d3bd85c,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,822 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509285962, Backend time ns: 54448114
2022-09-29T22:00:38,822 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509285962, Backend time ns: 54448114
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:509|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038822
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038822
2022-09-29T22:00:38,823 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:77.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b44033f9-984c-4842-967f-5bcf08350758,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:77.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b44033f9-984c-4842-967f-5bcf08350758,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,902 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 510033169, Backend time ns: 79608457
2022-09-29T22:00:38,902 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 510033169, Backend time ns: 79608457
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:510|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038902
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038902
2022-09-29T22:00:38,903 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:61.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5102c7a3-0ff9-4383-8a3f-c1e6e3b4f759,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 593
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:61.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5102c7a3-0ff9-4383-8a3f-c1e6e3b4f759,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,965 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529626013, Backend time ns: 62928620
2022-09-29T22:00:38,965 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529626013, Backend time ns: 62928620
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:529|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038965
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038965
2022-09-29T22:00:38,966 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.63|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3804e67f-da2a-4a75-873a-e4e6964861d2,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3804e67f-da2a-4a75-873a-e4e6964861d2,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,017 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 538078906, Backend time ns: 51358546
2022-09-29T22:00:39,017 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 538078906, Backend time ns: 51358546
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:538|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039017
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039017
2022-09-29T22:00:39,018 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a257e1fb-3f54-4db2-8e14-9f10d8b705cc,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a257e1fb-3f54-4db2-8e14-9f10d8b705cc,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,073 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 533661068, Backend time ns: 56003584
2022-09-29T22:00:39,073 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 533661068, Backend time ns: 56003584
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:533|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039073
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039073
2022-09-29T22:00:39,074 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc1d3d6c-0b39-4f30-856c-a3b72fd7acaf,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc1d3d6c-0b39-4f30-856c-a3b72fd7acaf,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 591
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,128 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535861723, Backend time ns: 55272304
2022-09-29T22:00:39,128 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535861723, Backend time ns: 55272304
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,129 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039129
2022-09-29T22:00:39,129 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039129
2022-09-29T22:00:39,130 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,201 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:71.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:61f8e8b1-fdcc-448e-95b6-63438e317708,timestamp:1664469039
2022-09-29T22:00:39,201 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:71.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:61f8e8b1-fdcc-448e-95b6-63438e317708,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 611
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,202 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537374055, Backend time ns: 73331646
2022-09-29T22:00:39,202 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537374055, Backend time ns: 73331646
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039202
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039202
2022-09-29T22:00:39,203 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f9e24769-d837-4a3d-abb2-c190693f6061,timestamp:1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f9e24769-d837-4a3d-abb2-c190693f6061,timestamp:1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 604
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,252 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554676771, Backend time ns: 49553649
2022-09-29T22:00:39,252 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554676771, Backend time ns: 49553649
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039252
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039252
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:868ddeeb-5c06-49e0-bf60-b2a6043ae72c,timestamp:1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 589
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:868ddeeb-5c06-49e0-bf60-b2a6043ae72c,timestamp:1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,304 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536954749, Backend time ns: 52616289
2022-09-29T22:00:39,304 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536954749, Backend time ns: 52616289
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:536|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039305
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039305
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:63.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e7773062-8021-46de-b714-464e1df16a8b,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 601
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:63.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e7773062-8021-46de-b714-464e1df16a8b,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,370 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535501648, Backend time ns: 65172323
2022-09-29T22:00:39,370 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535501648, Backend time ns: 65172323
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039370
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039370
2022-09-29T22:00:39,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4756e13-7e75-4a69-bde7-8d03756a11ca,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4756e13-7e75-4a69-bde7-8d03756a11ca,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 599
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546181103, Backend time ns: 52085772
2022-09-29T22:00:39,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546181103, Backend time ns: 52085772
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:546|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039422
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039422
2022-09-29T22:00:39,423 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21bad38d-92f2-458f-bb6f-77227df6f896,timestamp:1664469039
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 571
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21bad38d-92f2-458f-bb6f-77227df6f896,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,475 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519121250, Backend time ns: 52328750
2022-09-29T22:00:39,475 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519121250, Backend time ns: 52328750
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:519|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039475
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039475
2022-09-29T22:00:39,476 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.04|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9ac766e-ac2a-4793-b520-30dd492602bc,timestamp:1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 575
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.08|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9ac766e-ac2a-4793-b520-30dd492602bc,timestamp:1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508629468, Backend time ns: 66685395
2022-09-29T22:00:39,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508629468, Backend time ns: 66685395
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:508|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039542
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039542
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3301fad-b465-48b9-a5c6-e06fd85a509a,timestamp:1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 598
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:73.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3301fad-b465-48b9-a5c6-e06fd85a509a,timestamp:1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523750554, Backend time ns: 74769081
2022-09-29T22:00:39,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523750554, Backend time ns: 74769081
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039617
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039617
2022-09-29T22:00:39,618 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3cb1a500-a384-4d7b-bcde-ec73efba8b8b,timestamp:1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3cb1a500-a384-4d7b-bcde-ec73efba8b8b,timestamp:1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 599
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 542743698, Backend time ns: 56797676
2022-09-29T22:00:39,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 542743698, Backend time ns: 56797676
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:542|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039674
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039674
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1e1645a7-6a25-4978-b7a2-312caaa3befd,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 598
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1e1645a7-6a25-4978-b7a2-312caaa3befd,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 544255740, Backend time ns: 53170745
2022-09-29T22:00:39,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 544255740, Backend time ns: 53170745
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:544|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039727
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039727
2022-09-29T22:00:39,728 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.75|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:87894601-6d18-45f2-9314-1af240ddcef6,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:87894601-6d18-45f2-9314-1af240ddcef6,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 577
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,780 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523854419, Backend time ns: 53140611
2022-09-29T22:00:39,780 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523854419, Backend time ns: 53140611
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039780
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039780
2022-09-29T22:00:39,781 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:32b6ca04-75dd-4fa0-aeba-bb73c653c8f6,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:32b6ca04-75dd-4fa0-aeba-bb73c653c8f6,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 586
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,838 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527598274, Backend time ns: 57626782
2022-09-29T22:00:39,838 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527598274, Backend time ns: 57626782
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:527|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039838
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039838
2022-09-29T22:00:39,839 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1edc7f81-668c-4eda-9a58-f4beba63341a,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.77|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1edc7f81-668c-4eda-9a58-f4beba63341a,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 586
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,891 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 532463659, Backend time ns: 53034813
2022-09-29T22:00:39,891 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 532463659, Backend time ns: 53034813
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:532|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039891
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039891
2022-09-29T22:00:39,892 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9584a337-3b49-4f5a-9e47-5a9981f0d8ff,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 588
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9584a337-3b49-4f5a-9e47-5a9981f0d8ff,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,959 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 520492174, Backend time ns: 67605624
2022-09-29T22:00:39,959 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 520492174, Backend time ns: 67605624
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:520|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039959
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039959
2022-09-29T22:00:39,960 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21c19765-4884-4bf7-900d-72461b3a3712,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21c19765-4884-4bf7-900d-72461b3a3712,timestamp:1664469040
2022-09-29T22:00:40,013 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535954564, Backend time ns: 53767613
2022-09-29T22:00:40,013 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535954564, Backend time ns: 53767613
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040013
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040013
2022-09-29T22:00:40,014 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a020f9e-1928-4fc8-a639-40b67219ebb6,timestamp:1664469040
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a020f9e-1928-4fc8-a639-40b67219ebb6,timestamp:1664469040
2022-09-29T22:00:40,066 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537158363, Backend time ns: 53399187
2022-09-29T22:00:40,066 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537158363, Backend time ns: 53399187
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040067
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040067
2022-09-29T22:00:40,068 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:68523287-215d-44cf-b4bd-ad0bbbd06943,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:68523287-215d-44cf-b4bd-ad0bbbd06943,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 578
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,121 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523789094, Backend time ns: 54574267
2022-09-29T22:00:40,121 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523789094, Backend time ns: 54574267
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040121
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040121
2022-09-29T22:00:40,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb0cd80f-e817-40af-93bc-a4dd67226f59,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb0cd80f-e817-40af-93bc-a4dd67226f59,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 557
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,175 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502795831, Backend time ns: 53560670
2022-09-29T22:00:40,175 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502795831, Backend time ns: 53560670
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:502|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040175
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040175
2022-09-29T22:00:40,176 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.63|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e24e3f4-20d6-4d1f-b812-ec3b1580f195,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e24e3f4-20d6-4d1f-b812-ec3b1580f195,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,229 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500629622, Backend time ns: 54165687
2022-09-29T22:00:40,229 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500629622, Backend time ns: 54165687
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:500|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040230
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040230
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:34a5cb74-e5a8-4642-869a-38b4d75d6999,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:34a5cb74-e5a8-4642-869a-38b4d75d6999,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 557
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,285 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501651223, Backend time ns: 55642488
2022-09-29T22:00:40,285 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501651223, Backend time ns: 55642488
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040285
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040285
2022-09-29T22:00:40,286 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eb222e8b-2e81-4941-b17e-4a561a62530a,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eb222e8b-2e81-4941-b17e-4a561a62530a,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 559
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,340 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504186718, Backend time ns: 55045184
2022-09-29T22:00:40,340 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504186718, Backend time ns: 55045184
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:504|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040341
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040341
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:85b0fc67-cf56-4f4d-b33d-6c2f802be3cd,timestamp:1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:85b0fc67-cf56-4f4d-b33d-6c2f802be3cd,timestamp:1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,394 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501698590, Backend time ns: 53831722
2022-09-29T22:00:40,394 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501698590, Backend time ns: 53831722
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040395
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040395
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f334577-52c3-414f-a204-a8517af1624a,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f334577-52c3-414f-a204-a8517af1624a,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,452 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502452830, Backend time ns: 57783154
2022-09-29T22:00:40,452 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502452830, Backend time ns: 57783154
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:502|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040452
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040452
2022-09-29T22:00:40,453 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a0917f22-1d95-4b8f-9fe5-c48b0dc47f86,timestamp:1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 546
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a0917f22-1d95-4b8f-9fe5-c48b0dc47f86,timestamp:1664469040
2022-09-29T22:00:40,506 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492284492, Backend time ns: 53830751
2022-09-29T22:00:40,506 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492284492, Backend time ns: 53830751
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040506
2022-09-29T22:00:40,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040506
2022-09-29T22:00:40,508 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:753ef7ef-9907-4154-ba41-8f807cfa7898,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:753ef7ef-9907-4154-ba41-8f807cfa7898,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 549
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,563 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492611789, Backend time ns: 56789514
2022-09-29T22:00:40,563 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492611789, Backend time ns: 56789514
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040563
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040563
2022-09-29T22:00:40,564 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ae35afc-3815-495e-911d-9163f565cafd,timestamp:1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ae35afc-3815-495e-911d-9163f565cafd,timestamp:1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 559
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,628 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 495491162, Backend time ns: 64046065
2022-09-29T22:00:40,628 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 495491162, Backend time ns: 64046065
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:495|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040628
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040628
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fa7631f6-36c5-4e0e-83dd-e3e29b3ce3cd,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fa7631f6-36c5-4e0e-83dd-e3e29b3ce3cd,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 566
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,688 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505369484, Backend time ns: 60499603
2022-09-29T22:00:40,688 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505369484, Backend time ns: 60499603
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:505|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040688
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040688
2022-09-29T22:00:40,690 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 68
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:67.07|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2a0b5d57-567e-4684-aade-f0a77dd328a0,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 68
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:67.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2a0b5d57-567e-4684-aade-f0a77dd328a0,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 582
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,758 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 512312529, Backend time ns: 69655565
2022-09-29T22:00:40,758 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 512312529, Backend time ns: 69655565
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:512|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040758
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040758
2022-09-29T22:00:40,759 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05af3ef6-d310-4fa4-a096-d75f01a71684,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.69|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05af3ef6-d310-4fa4-a096-d75f01a71684,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,809 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527777793, Backend time ns: 51242446
2022-09-29T22:00:40,809 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527777793, Backend time ns: 51242446
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:527|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040810
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040810
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:683a7f54-d317-402d-9092-ff39c0e2b105,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:683a7f54-d317-402d-9092-ff39c0e2b105,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523226946, Backend time ns: 55212208
2022-09-29T22:00:40,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523226946, Backend time ns: 55212208
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040865
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040865
2022-09-29T22:00:40,866 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c2ddc397-dfcb-43c4-85ff-69cf1c2b1250,timestamp:1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c2ddc397-dfcb-43c4-85ff-69cf1c2b1250,timestamp:1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,931 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523417233, Backend time ns: 66616425
2022-09-29T22:00:40,931 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523417233, Backend time ns: 66616425
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040932
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040932
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5079721a-4796-44f2-b206-6209671a2480,timestamp:1664469040
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 597
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5079721a-4796-44f2-b206-6209671a2480,timestamp:1664469040
2022-09-29T22:00:40,993 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536146652, Backend time ns: 60905978
2022-09-29T22:00:40,993 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536146652, Backend time ns: 60905978
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:536|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040993
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040993
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.71|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77af4fab-fe9d-404c-b05f-b5bca04bb7fb,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77af4fab-fe9d-404c-b05f-b5bca04bb7fb,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 612
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,065 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 539355724, Backend time ns: 72034328
2022-09-29T22:00:41,065 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 539355724, Backend time ns: 72034328
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:539|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041065
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041065
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d0a935e-bef2-4ec5-9cf1-74bf7c714c98,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d0a935e-bef2-4ec5-9cf1-74bf7c714c98,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 611
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,118 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 557279149, Backend time ns: 53466209
2022-09-29T22:00:41,118 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 557279149, Backend time ns: 53466209
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:557|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041118
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041118
2022-09-29T22:00:41,120 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,182 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,182 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:aaa008bd-233e-440a-a1f0-7f80154b944d,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:aaa008bd-233e-440a-a1f0-7f80154b944d,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 619
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,183 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 553890443, Backend time ns: 64380345
2022-09-29T22:00:41,183 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 553890443, Backend time ns: 64380345
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:553|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041183
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041183
2022-09-29T22:00:41,184 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1662a337-a89a-4348-90d0-01e51258c290,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1662a337-a89a-4348-90d0-01e51258c290,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 608
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,236 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554514136, Backend time ns: 53397135
2022-09-29T22:00:41,236 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554514136, Backend time ns: 53397135
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,237 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041237
2022-09-29T22:00:41,237 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041237
2022-09-29T22:00:41,238 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,289 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:41,289 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5eb2991b-10ac-4c3a-9ef0-1e945d1122cf,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5eb2991b-10ac-4c3a-9ef0-1e945d1122cf,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 601
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546976790, Backend time ns: 53346101
2022-09-29T22:00:41,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546976790, Backend time ns: 53346101
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:546|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041290
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041290
2022-09-29T22:00:41,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7479b24b-a453-4ce4-b7f9-fb8fd7cce1c0,timestamp:1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7479b24b-a453-4ce4-b7f9-fb8fd7cce1c0,timestamp:1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 602
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,361 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 530961435, Backend time ns: 71477374
2022-09-29T22:00:41,361 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 530961435, Backend time ns: 71477374
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:530|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041362
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041362
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bc748913-eb54-43b8-921e-2490c2634fe7,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bc748913-eb54-43b8-921e-2490c2634fe7,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 612
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 551109512, Backend time ns: 60680126
2022-09-29T22:00:41,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 551109512, Backend time ns: 60680126
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:551|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041422
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041422
2022-09-29T22:00:41,423 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9eb8972a-1d68-4d38-b81e-f3958474b083,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 632
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9eb8972a-1d68-4d38-b81e-f3958474b083,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,498 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556567568, Backend time ns: 75441992
2022-09-29T22:00:41,498 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556567568, Backend time ns: 75441992
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:556|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041498
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041498
2022-09-29T22:00:41,499 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.58|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:63d7a9ee-5a4c-4c67-afcb-27d0923d16a3,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 616
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:63d7a9ee-5a4c-4c67-afcb-27d0923d16a3,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 565398167, Backend time ns: 50576528
2022-09-29T22:00:41,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 565398167, Backend time ns: 50576528
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:565|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041549
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041549
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4028ea9c-b9e1-471f-ae46-253f7bb0d58c,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:69.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4028ea9c-b9e1-471f-ae46-253f7bb0d58c,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 627
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 555134781, Backend time ns: 71198078
2022-09-29T22:00:41,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 555134781, Backend time ns: 71198078
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:555|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041620
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041620
2022-09-29T22:00:41,621 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:61.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b782014-42a0-449b-b6cb-c08ad679240d,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:61.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b782014-42a0-449b-b6cb-c08ad679240d,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 617
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,683 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554300266, Backend time ns: 62763393
2022-09-29T22:00:41,683 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554300266, Backend time ns: 62763393
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041683
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041683
2022-09-29T22:00:41,684 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f93aa80-3eb9-4cb3-88c7-ca1b64494ac4,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f93aa80-3eb9-4cb3-88c7-ca1b64494ac4,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 622
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,741 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563673556, Backend time ns: 57707611
2022-09-29T22:00:41,741 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563673556, Backend time ns: 57707611
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:563|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041741
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041741
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:41bd749b-ae2b-47bc-8ca6-bee6d618182c,timestamp:1664469041
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 628
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:41bd749b-ae2b-47bc-8ca6-bee6d618182c,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556880524, Backend time ns: 71770531
2022-09-29T22:00:41,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556880524, Backend time ns: 71770531
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:556|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041813
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041813
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eeefca7c-d165-40cf-a020-da743dc8b9c9,timestamp:1664469041
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 625
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eeefca7c-d165-40cf-a020-da743dc8b9c9,timestamp:1664469041
2022-09-29T22:00:41,862 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 575337827, Backend time ns: 49742678
2022-09-29T22:00:41,862 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 575337827, Backend time ns: 49742678
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:575|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041863
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041863
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b95a7065-3685-428a-b5a7-8cab09c0bcc7,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 635
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,926 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 571571363, Backend time ns: 63663022
2022-09-29T22:00:41,926 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 571571363, Backend time ns: 63663022
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b95a7065-3685-428a-b5a7-8cab09c0bcc7,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:571|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041926
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041926
2022-09-29T22:00:41,927 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.29|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5340de69-f4d5-41aa-9bee-3bde6833ff00,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 616
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5340de69-f4d5-41aa-9bee-3bde6833ff00,timestamp:1664469041
2022-09-29T22:00:41,978 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563864296, Backend time ns: 51385280
2022-09-29T22:00:41,978 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563864296, Backend time ns: 51385280
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:563|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041978
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041978
2022-09-29T22:00:41,979 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:74d7b011-de35-47f4-9b72-5c77dd92229b,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 608
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:74d7b011-de35-47f4-9b72-5c77dd92229b,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,031 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554746364, Backend time ns: 53135430
2022-09-29T22:00:42,031 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554746364, Backend time ns: 53135430
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042031
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042031
2022-09-29T22:00:42,032 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b54bdf48-f129-498b-b7b0-a357802bcb93,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b54bdf48-f129-498b-b7b0-a357802bcb93,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 588
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 531894832, Backend time ns: 55646645
2022-09-29T22:00:42,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 531894832, Backend time ns: 55646645
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:531|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042087
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042087
2022-09-29T22:00:42,088 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21154f13-0902-4d86-8dc2-7a80f890a4c1,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.84|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21154f13-0902-4d86-8dc2-7a80f890a4c1,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,140 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537273904, Backend time ns: 52911870
2022-09-29T22:00:42,140 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537273904, Backend time ns: 52911870
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042140
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042140
2022-09-29T22:00:42,141 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dde6b1a3-6cf2-48b2-a69d-58232f6b5757,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.83|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dde6b1a3-6cf2-48b2-a69d-58232f6b5757,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 576
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519141247, Backend time ns: 57019860
2022-09-29T22:00:42,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519141247, Backend time ns: 57019860
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:519|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042197
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042197
2022-09-29T22:00:42,198 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.55|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05c52c23-973c-4693-9527-0a7cc6ad4f04,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05c52c23-973c-4693-9527-0a7cc6ad4f04,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 571
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,255 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513571877, Backend time ns: 57758451
2022-09-29T22:00:42,255 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513571877, Backend time ns: 57758451
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:513|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042255
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042255
2022-09-29T22:00:42,256 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.78|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:de635535-f211-4c67-93aa-6f0406093dd7,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:de635535-f211-4c67-93aa-6f0406093dd7,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 567
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,308 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513615334, Backend time ns: 52831536
2022-09-29T22:00:42,308 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513615334, Backend time ns: 52831536
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:513|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,312 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:00:42,312 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:00:42,313 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,313 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Frontend disconnected.
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:00:42,333 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:00:42,333 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:00:42,340 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,340 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,344 [INFO ] epollEventLoopGroup-3-23 ACCESS_LOG - /127.0.0.1:58116 "DELETE /models/benchmark HTTP/1.1" 200 32
2022-09-29T22:00:42,344 [INFO ] epollEventLoopGroup-3-23 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
