2022-09-29T22:00:20,584 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:00:20,584 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:00:20,719 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:00:20,719 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:00:20,730 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:00:20,730 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:00:20,763 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:00:20,763 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:00:20,827 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:00:20,828 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:00:20,829 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:00:20,829 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:00:21,120 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,121 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:163.31096649169922|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:57.954715728759766|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:26.2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,122 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:3384.48828125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,123 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3423.6328125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:21,123 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:56.7|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469021
2022-09-29T22:00:22,945 [INFO ] pool-2-thread-1 ACCESS_LOG - /127.0.0.1:37808 "GET /ping HTTP/1.1" 200 7
2022-09-29T22:00:22,946 [INFO ] pool-2-thread-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:34,200 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:00:34,200 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:00:34,202 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:00:34,202 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:00:34,202 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:00:34,202 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:00:34,204 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:00:34,204 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:00:34,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:00:34,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:00:34,831 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-09-29T22:00:34,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - [PID]19736
2022-09-29T22:00:34,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Torch worker started.
2022-09-29T22:00:34,833 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Python runtime: 3.10.6
2022-09-29T22:00:34,833 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:00:34,833 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:00:34,837 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:00:34,837 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:00:34,843 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-09-29T22:00:34,845 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469034845
2022-09-29T22:00:34,845 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469034845
2022-09-29T22:00:34,854 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - model_name: benchmark, batchSize: 1
2022-09-29T22:00:35,715 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 861
2022-09-29T22:00:35,715 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 861
2022-09-29T22:00:35,716 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:00:35,716 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:00:35,716 [INFO ] W-9000-benchmark_1.0 TS_METRICS - W-9000-benchmark_1.0.ms:1507|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,716 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,716 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:35646 "POST /models?model_name=benchmark&url=https%3A%2F%2Ftorchserve.pytorch.org%2Fmar_files%2Fresnet-18.mar&batch_delay=200&batch_size=1&initial_workers=1&synchronous=true HTTP/1.1" 200 12755
2022-09-29T22:00:35,716 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,745 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035745
2022-09-29T22:00:35,745 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035745
2022-09-29T22:00:35,748 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:120.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:38bc511b-3e14-4428-9c20-3925855817dc,timestamp:1664469035
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58898 "POST /predictions/benchmark HTTP/1.0" 200 129
2022-09-29T22:00:35,869 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:38bc511b-3e14-4428-9c20-3925855817dc,timestamp:1664469035
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,870 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 141253, Backend time ns: 125001630
2022-09-29T22:00:35,870 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 141253, Backend time ns: 125001630
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,870 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035880
2022-09-29T22:00:35,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035880
2022-09-29T22:00:35,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:85.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:59ce105d-57ce-4f3b-af08-f632844ddc17,timestamp:1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:85.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:59ce105d-57ce-4f3b-af08-f632844ddc17,timestamp:1664469035
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58898 "POST /predictions/benchmark HTTP/1.0" 200 91
2022-09-29T22:00:35,967 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:35,968 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84500, Backend time ns: 88006739
2022-09-29T22:00:35,968 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84500, Backend time ns: 88006739
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469035
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035968
2022-09-29T22:00:35,968 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469035968
2022-09-29T22:00:35,969 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469035
2022-09-29T22:00:36,020 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,020 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0eef0230-4bb9-4136-ba0c-3ad291d674df,timestamp:1664469036
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58910 "POST /predictions/benchmark HTTP/1.0" 200 127
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0eef0230-4bb9-4136-ba0c-3ad291d674df,timestamp:1664469036
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,021 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 73904132, Backend time ns: 53019771
2022-09-29T22:00:36,021 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 73904132, Backend time ns: 53019771
2022-09-29T22:00:36,021 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:73|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036022
2022-09-29T22:00:36,022 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036022
2022-09-29T22:00:36,023 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:36,077 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:49465d44-151b-4699-83c4-6ba33754e400,timestamp:1664469036
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:49465d44-151b-4699-83c4-6ba33754e400,timestamp:1664469036
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58916 "POST /predictions/benchmark HTTP/1.0" 200 171
2022-09-29T22:00:36,078 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 114451859, Backend time ns: 56477272
2022-09-29T22:00:36,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 114451859, Backend time ns: 56477272
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:114|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036079
2022-09-29T22:00:36,079 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036079
2022-09-29T22:00:36,080 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:36,131 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.0|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dba103ce-a070-4ddf-aa32-b937910945e7,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dba103ce-a070-4ddf-aa32-b937910945e7,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58946 "POST /predictions/benchmark HTTP/1.0" 200 223
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,132 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169202035, Backend time ns: 53081067
2022-09-29T22:00:36,132 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 169202035, Backend time ns: 53081067
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:169|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,133 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036133
2022-09-29T22:00:36,133 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036133
2022-09-29T22:00:36,134 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b342e73e-fd5f-4da9-97ed-7abc2ca5163a,timestamp:1664469036
2022-09-29T22:00:36,185 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58926 "POST /predictions/benchmark HTTP/1.0" 200 275
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b342e73e-fd5f-4da9-97ed-7abc2ca5163a,timestamp:1664469036
2022-09-29T22:00:36,186 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,187 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 221050540, Backend time ns: 54221582
2022-09-29T22:00:36,187 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 221050540, Backend time ns: 54221582
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:221|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036187
2022-09-29T22:00:36,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036187
2022-09-29T22:00:36,188 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58956 "POST /predictions/benchmark HTTP/1.0" 200 318
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,241 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264579777, Backend time ns: 53559648
2022-09-29T22:00:36,241 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 264579777, Backend time ns: 53559648
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:264|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036241
2022-09-29T22:00:36,241 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036241
2022-09-29T22:00:36,240 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e66e1183-e7de-4744-b567-ba5d2232f923,timestamp:1664469036
2022-09-29T22:00:36,242 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e66e1183-e7de-4744-b567-ba5d2232f923,timestamp:1664469036
2022-09-29T22:00:36,243 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bb8df1c3-be68-4967-8e7b-3c74bff2652b,timestamp:1664469036
2022-09-29T22:00:36,294 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bb8df1c3-be68-4967-8e7b-3c74bff2652b,timestamp:1664469036
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58934 "POST /predictions/benchmark HTTP/1.0" 200 370
2022-09-29T22:00:36,295 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,296 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 315628077, Backend time ns: 54122336
2022-09-29T22:00:36,296 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 315628077, Backend time ns: 54122336
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:315|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036296
2022-09-29T22:00:36,296 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036296
2022-09-29T22:00:36,297 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:68.71|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35bac8d2-73b3-4a8b-9b63-8c9c07ce3bff,timestamp:1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:68.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35bac8d2-73b3-4a8b-9b63-8c9c07ce3bff,timestamp:1664469036
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58972 "POST /predictions/benchmark HTTP/1.0" 200 439
2022-09-29T22:00:36,366 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,367 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 368358097, Backend time ns: 70617620
2022-09-29T22:00:36,367 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 368358097, Backend time ns: 70617620
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:368|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036367
2022-09-29T22:00:36,367 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036367
2022-09-29T22:00:36,368 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:36,418 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:73d44da5-bf66-4d8a-ba04-d8e04f209fdb,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58918 "POST /predictions/benchmark HTTP/1.0" 200 489
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:73d44da5-bf66-4d8a-ba04-d8e04f209fdb,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,419 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 432300999, Backend time ns: 51937954
2022-09-29T22:00:36,419 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 432300999, Backend time ns: 51937954
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:432|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,431 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036431
2022-09-29T22:00:36,431 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036431
2022-09-29T22:00:36,432 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:80.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d5ec54f6-c5c5-4451-83b8-03653a1fd5d9,timestamp:1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 83
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:80.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d5ec54f6-c5c5-4451-83b8-03653a1fd5d9,timestamp:1664469036
2022-09-29T22:00:36,513 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,514 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 64023, Backend time ns: 83015632
2022-09-29T22:00:36,514 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 64023, Backend time ns: 83015632
2022-09-29T22:00:36,514 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,514 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036519
2022-09-29T22:00:36,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036519
2022-09-29T22:00:36,520 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,595 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9fe3ad91-d7ef-4c77-b437-8313fa84d5ee,timestamp:1664469036
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9fe3ad91-d7ef-4c77-b437-8313fa84d5ee,timestamp:1664469036
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 78
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,596 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 223476, Backend time ns: 77292470
2022-09-29T22:00:36,596 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 223476, Backend time ns: 77292470
2022-09-29T22:00:36,596 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036597
2022-09-29T22:00:36,597 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036597
2022-09-29T22:00:36,598 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f48a73f2-1d05-4b39-b4be-85d55f2957dd,timestamp:1664469036
2022-09-29T22:00:36,653 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f48a73f2-1d05-4b39-b4be-85d55f2957dd,timestamp:1664469036
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 130
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,654 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 72249218, Backend time ns: 57296292
2022-09-29T22:00:36,654 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 72249218, Backend time ns: 57296292
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:72|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,654 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,655 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036655
2022-09-29T22:00:36,655 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036655
2022-09-29T22:00:36,656 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.46|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:894b13bc-3cb9-4fb3-9790-461b9615bdfb,timestamp:1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:894b13bc-3cb9-4fb3-9790-461b9615bdfb,timestamp:1664469036
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 181
2022-09-29T22:00:36,709 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,710 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 125724380, Backend time ns: 54944451
2022-09-29T22:00:36,710 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 125724380, Backend time ns: 54944451
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:125|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036710
2022-09-29T22:00:36,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036710
2022-09-29T22:00:36,711 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,763 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9e8859b-4460-4939-9bc0-6e096d9dae33,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 229
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9e8859b-4460-4939-9bc0-6e096d9dae33,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,764 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 174683287, Backend time ns: 54086360
2022-09-29T22:00:36,764 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 174683287, Backend time ns: 54086360
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:174|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,764 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,765 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036764
2022-09-29T22:00:36,765 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036764
2022-09-29T22:00:36,766 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d504bdbe-c8c9-419b-8471-bb1437cf68cf,timestamp:1664469036
2022-09-29T22:00:36,817 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 281
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.85|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d504bdbe-c8c9-419b-8471-bb1437cf68cf,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,818 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 228091381, Backend time ns: 53238757
2022-09-29T22:00:36,818 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 228091381, Backend time ns: 53238757
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:228|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036818
2022-09-29T22:00:36,818 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036818
2022-09-29T22:00:36,820 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:89bbcdd7-9af0-4234-9254-740845e40e2e,timestamp:1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:89bbcdd7-9af0-4234-9254-740845e40e2e,timestamp:1664469036
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 359
2022-09-29T22:00:36,896 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,897 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 281095630, Backend time ns: 78310320
2022-09-29T22:00:36,897 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 281095630, Backend time ns: 78310320
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:281|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036897
2022-09-29T22:00:36,897 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036897
2022-09-29T22:00:36,898 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:56f6fd8c-d9ce-4ad4-b439-78dea3912aa1,timestamp:1664469036
2022-09-29T22:00:36,950 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:56f6fd8c-d9ce-4ad4-b439-78dea3912aa1,timestamp:1664469036
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 413
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:36,951 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 358505946, Backend time ns: 54077998
2022-09-29T22:00:36,951 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 358505946, Backend time ns: 54077998
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:358|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,951 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469036
2022-09-29T22:00:36,952 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036951
2022-09-29T22:00:36,952 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469036951
2022-09-29T22:00:36,953 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469036
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:504ad887-77cc-45c5-a7b2-726e813ebcdd,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:504ad887-77cc-45c5-a7b2-726e813ebcdd,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 464
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 408745506, Backend time ns: 54687967
2022-09-29T22:00:37,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 408745506, Backend time ns: 54687967
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:408|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,007 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037007
2022-09-29T22:00:37,007 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037007
2022-09-29T22:00:37,008 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:facd1925-bb98-4144-ae11-0a77ca6e3690,timestamp:1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 508
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:facd1925-bb98-4144-ae11-0a77ca6e3690,timestamp:1664469037
2022-09-29T22:00:37,060 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,061 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 454254255, Backend time ns: 53969238
2022-09-29T22:00:37,061 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 454254255, Backend time ns: 53969238
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:454|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037061
2022-09-29T22:00:37,061 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037061
2022-09-29T22:00:37,062 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:981b1548-4cfc-4e05-b6f8-5568b846f124,timestamp:1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 61
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:981b1548-4cfc-4e05-b6f8-5568b846f124,timestamp:1664469037
2022-09-29T22:00:37,122 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 566
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,124 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 503681605, Backend time ns: 63126022
2022-09-29T22:00:37,124 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 503681605, Backend time ns: 63126022
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:503|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037124
2022-09-29T22:00:37,124 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037124
2022-09-29T22:00:37,125 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a41be723-c409-4882-999d-a0ca70f6bbbc,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.83|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a41be723-c409-4882-999d-a0ca70f6bbbc,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 580
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,178 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 525506829, Backend time ns: 53978552
2022-09-29T22:00:37,178 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 525506829, Backend time ns: 53978552
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:525|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,178 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,179 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037179
2022-09-29T22:00:37,179 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037179
2022-09-29T22:00:37,181 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.98|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:66d0b088-1d74-44e5-88ee-5532cfc3c852,timestamp:1664469037
2022-09-29T22:00:37,233 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:66d0b088-1d74-44e5-88ee-5532cfc3c852,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 578
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,234 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 521708272, Backend time ns: 55483905
2022-09-29T22:00:37,234 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 521708272, Backend time ns: 55483905
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:521|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037234
2022-09-29T22:00:37,234 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037234
2022-09-29T22:00:37,235 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bd362d7a-baa6-4ae3-8e4d-9598c5ace953,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bd362d7a-baa6-4ae3-8e4d-9598c5ace953,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,291 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522344641, Backend time ns: 56833551
2022-09-29T22:00:37,291 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522344641, Backend time ns: 56833551
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037291
2022-09-29T22:00:37,291 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037291
2022-09-29T22:00:37,292 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.39|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:75536641-2dbc-4ae3-8c41-8e2eaffe0a90,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:75536641-2dbc-4ae3-8c41-8e2eaffe0a90,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 580
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,346 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 524611094, Backend time ns: 54601801
2022-09-29T22:00:37,346 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 524611094, Backend time ns: 54601801
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:524|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037346
2022-09-29T22:00:37,346 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037346
2022-09-29T22:00:37,347 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f843cdda-5b9b-463b-ae40-fe25d6166182,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f843cdda-5b9b-463b-ae40-fe25d6166182,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 581
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,400 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 526769833, Backend time ns: 53934595
2022-09-29T22:00:37,400 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 526769833, Backend time ns: 53934595
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:526|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,400 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037400
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037400
2022-09-29T22:00:37,401 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:293c989b-dfc2-4906-be67-393544c26a95,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:293c989b-dfc2-4906-be67-393544c26a95,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,454 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501569208, Backend time ns: 53748109
2022-09-29T22:00:37,454 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501569208, Backend time ns: 53748109
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,454 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037454
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037454
2022-09-29T22:00:37,455 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,507 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:91bb368f-6a57-48b9-b5be-782e575e01bb,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:91bb368f-6a57-48b9-b5be-782e575e01bb,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,508 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501662253, Backend time ns: 53280897
2022-09-29T22:00:37,508 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501662253, Backend time ns: 53280897
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037508
2022-09-29T22:00:37,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037508
2022-09-29T22:00:37,509 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:395e08db-836c-45fc-b5b1-e7b6f785b66b,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:395e08db-836c-45fc-b5b1-e7b6f785b66b,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 558
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,566 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 499906854, Backend time ns: 58355639
2022-09-29T22:00:37,566 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 499906854, Backend time ns: 58355639
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:499|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,566 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037567
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037567
2022-09-29T22:00:37,567 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:be01102e-f31e-4238-ac65-ef71a76c8e34,timestamp:1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:be01102e-f31e-4238-ac65-ef71a76c8e34,timestamp:1664469037
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:37,619 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 558
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 486132656, Backend time ns: 53283862
2022-09-29T22:00:37,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 486132656, Backend time ns: 53283862
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:486|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037620
2022-09-29T22:00:37,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037620
2022-09-29T22:00:37,621 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7e926a4f-ccec-4e28-9c83-a2ad342d54ea,timestamp:1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7e926a4f-ccec-4e28-9c83-a2ad342d54ea,timestamp:1664469037
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 548
2022-09-29T22:00:37,673 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494579267, Backend time ns: 53412777
2022-09-29T22:00:37,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494579267, Backend time ns: 53412777
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:494|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037674
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037674
2022-09-29T22:00:37,674 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7f7b6e6-bfe4-45c1-8dac-2a34de999cc3,timestamp:1664469037
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 547
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 493488637, Backend time ns: 53700670
2022-09-29T22:00:37,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7f7b6e6-bfe4-45c1-8dac-2a34de999cc3,timestamp:1664469037
2022-09-29T22:00:37,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 493488637, Backend time ns: 53700670
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:493|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037728
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037728
2022-09-29T22:00:37,728 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:58.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7d2d39ce-623b-461f-adf4-da97ebcda3e6,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 552
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,787 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492129145, Backend time ns: 59468059
2022-09-29T22:00:37,787 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492129145, Backend time ns: 59468059
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:58.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7d2d39ce-623b-461f-adf4-da97ebcda3e6,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037787
2022-09-29T22:00:37,787 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037787
2022-09-29T22:00:37,788 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:cbdc37e0-5a7a-49aa-81eb-49c58000a4df,timestamp:1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:cbdc37e0-5a7a-49aa-81eb-49c58000a4df,timestamp:1664469037
2022-09-29T22:00:37,864 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494642842, Backend time ns: 77327067
2022-09-29T22:00:37,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 494642842, Backend time ns: 77327067
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:494|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037865
2022-09-29T22:00:37,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037865
2022-09-29T22:00:37,866 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,918 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,918 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0b591f68-952e-4755-8d2f-ae24956f6e36,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.96|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0b591f68-952e-4755-8d2f-ae24956f6e36,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,919 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517421167, Backend time ns: 53964487
2022-09-29T22:00:37,919 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517421167, Backend time ns: 53964487
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037919
2022-09-29T22:00:37,919 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037919
2022-09-29T22:00:37,920 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:37,972 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8f35ea11-97bc-4c93-a36a-8730b28d58d1,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 572
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.65|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8f35ea11-97bc-4c93-a36a-8730b28d58d1,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:37,973 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517562220, Backend time ns: 53476619
2022-09-29T22:00:37,973 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517562220, Backend time ns: 53476619
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469037
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037973
2022-09-29T22:00:37,973 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469037973
2022-09-29T22:00:37,974 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469037
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a9c43b7c-1c46-4166-a402-319ecc7ab293,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a9c43b7c-1c46-4166-a402-319ecc7ab293,timestamp:1664469038
2022-09-29T22:00:38,027 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 573
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517274971, Backend time ns: 55092960
2022-09-29T22:00:38,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 517274971, Backend time ns: 55092960
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:517|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038028
2022-09-29T22:00:38,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038028
2022-09-29T22:00:38,029 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6a391172-5f93-45eb-bcee-71ab299416bd,timestamp:1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6a391172-5f93-45eb-bcee-71ab299416bd,timestamp:1664469038
2022-09-29T22:00:38,083 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 574
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,084 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509694903, Backend time ns: 55537091
2022-09-29T22:00:38,084 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509694903, Backend time ns: 55537091
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:509|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038084
2022-09-29T22:00:38,084 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038084
2022-09-29T22:00:38,085 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:38,143 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:57.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e753d5a-a820-4f3a-8551-f0b6ea92f20b,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 575
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:57.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e753d5a-a820-4f3a-8551-f0b6ea92f20b,timestamp:1664469038
2022-09-29T22:00:38,144 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 516266339, Backend time ns: 59647722
2022-09-29T22:00:38,144 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 516266339, Backend time ns: 59647722
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:516|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038144
2022-09-29T22:00:38,144 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038144
2022-09-29T22:00:38,145 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d01f0f02-eb3e-4447-868e-e2f8676abf7c,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 576
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d01f0f02-eb3e-4447-868e-e2f8676abf7c,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522518552, Backend time ns: 53373652
2022-09-29T22:00:38,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522518552, Backend time ns: 53373652
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038197
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038197
2022-09-29T22:00:38,198 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:57.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:589c2ddb-3bcf-4f84-bedf-919d4ab572da,timestamp:1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:57.85|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:589c2ddb-3bcf-4f84-bedf-919d4ab572da,timestamp:1664469038
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 582
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,257 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522836378, Backend time ns: 59866002
2022-09-29T22:00:38,257 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522836378, Backend time ns: 59866002
2022-09-29T22:00:38,257 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038258
2022-09-29T22:00:38,258 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038258
2022-09-29T22:00:38,259 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ceffefe8-8c74-44eb-bbdc-800f964d88ad,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ceffefe8-8c74-44eb-bbdc-800f964d88ad,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 583
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,311 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529044912, Backend time ns: 53675043
2022-09-29T22:00:38,311 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529044912, Backend time ns: 53675043
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:529|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,311 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038312
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038312
2022-09-29T22:00:38,312 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 59
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:58.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc6f8917-efe0-4095-837c-f11cdab9547a,timestamp:1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:58.46|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc6f8917-efe0-4095-837c-f11cdab9547a,timestamp:1664469038
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 583
2022-09-29T22:00:38,371 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,372 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522979626, Backend time ns: 59939674
2022-09-29T22:00:38,372 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 522979626, Backend time ns: 59939674
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:522|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038372
2022-09-29T22:00:38,372 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038372
2022-09-29T22:00:38,373 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:af75d3c3-3a15-407b-944d-b8e020416f22,timestamp:1664469038
2022-09-29T22:00:38,425 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:af75d3c3-3a15-407b-944d-b8e020416f22,timestamp:1664469038
2022-09-29T22:00:38,426 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505609889, Backend time ns: 54146609
2022-09-29T22:00:38,426 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505609889, Backend time ns: 54146609
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:505|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038426
2022-09-29T22:00:38,426 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038426
2022-09-29T22:00:38,427 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:71c9af56-ac11-415a-ab3d-c8ac94cef4ab,timestamp:1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:71c9af56-ac11-415a-ab3d-c8ac94cef4ab,timestamp:1664469038
2022-09-29T22:00:38,481 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 561
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,482 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506065267, Backend time ns: 55560829
2022-09-29T22:00:38,482 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506065267, Backend time ns: 55560829
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:506|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038482
2022-09-29T22:00:38,482 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038482
2022-09-29T22:00:38,483 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4661708c-6c63-4c2c-a284-808f8f6cf11a,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4661708c-6c63-4c2c-a284-808f8f6cf11a,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 562
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,536 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508054233, Backend time ns: 53993017
2022-09-29T22:00:38,536 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508054233, Backend time ns: 53993017
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:508|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038536
2022-09-29T22:00:38,536 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038536
2022-09-29T22:00:38,537 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.0|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e9711fa-9ccf-46a9-af8b-a1edb31e1d7d,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 561
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2e9711fa-9ccf-46a9-af8b-a1edb31e1d7d,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,590 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506675718, Backend time ns: 53885026
2022-09-29T22:00:38,590 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 506675718, Backend time ns: 53885026
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:506|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038590
2022-09-29T22:00:38,590 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038590
2022-09-29T22:00:38,591 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5d8aa477-e8fa-4ae1-b958-9ab69b0cd302,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5d8aa477-e8fa-4ae1-b958-9ab69b0cd302,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,645 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504826551, Backend time ns: 55110865
2022-09-29T22:00:38,645 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504826551, Backend time ns: 55110865
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:504|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,645 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,646 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038646
2022-09-29T22:00:38,646 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038646
2022-09-29T22:00:38,647 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0f8ce9a-f591-4348-99ed-47858889b439,timestamp:1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 568
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0f8ce9a-f591-4348-99ed-47858889b439,timestamp:1664469038
2022-09-29T22:00:38,713 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,713 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500741620, Backend time ns: 67934413
2022-09-29T22:00:38,713 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500741620, Backend time ns: 67934413
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:500|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038714
2022-09-29T22:00:38,714 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038714
2022-09-29T22:00:38,715 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20eb7948-10ed-4a86-8704-4563c22f3637,timestamp:1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20eb7948-10ed-4a86-8704-4563c22f3637,timestamp:1664469038
2022-09-29T22:00:38,767 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 569
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,768 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 515257115, Backend time ns: 53979481
2022-09-29T22:00:38,768 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 515257115, Backend time ns: 53979481
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:515|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038768
2022-09-29T22:00:38,768 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038768
2022-09-29T22:00:38,769 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7df7177-8708-435b-887a-465b3d3bd85c,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 564
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f7df7177-8708-435b-887a-465b3d3bd85c,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,822 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509285962, Backend time ns: 54448114
2022-09-29T22:00:38,822 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 509285962, Backend time ns: 54448114
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:509|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038822
2022-09-29T22:00:38,822 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038822
2022-09-29T22:00:38,823 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:77.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b44033f9-984c-4842-967f-5bcf08350758,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:77.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b44033f9-984c-4842-967f-5bcf08350758,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,902 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 510033169, Backend time ns: 79608457
2022-09-29T22:00:38,902 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 510033169, Backend time ns: 79608457
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:510|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038902
2022-09-29T22:00:38,902 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038902
2022-09-29T22:00:38,903 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:61.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5102c7a3-0ff9-4383-8a3f-c1e6e3b4f759,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 593
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:61.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5102c7a3-0ff9-4383-8a3f-c1e6e3b4f759,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:38,965 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529626013, Backend time ns: 62928620
2022-09-29T22:00:38,965 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 529626013, Backend time ns: 62928620
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:529|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469038
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038965
2022-09-29T22:00:38,965 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469038965
2022-09-29T22:00:38,966 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469038
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 50
2022-09-29T22:00:39,016 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.63|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3804e67f-da2a-4a75-873a-e4e6964861d2,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3804e67f-da2a-4a75-873a-e4e6964861d2,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,017 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 538078906, Backend time ns: 51358546
2022-09-29T22:00:39,017 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 538078906, Backend time ns: 51358546
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:538|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039017
2022-09-29T22:00:39,017 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039017
2022-09-29T22:00:39,018 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a257e1fb-3f54-4db2-8e14-9f10d8b705cc,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a257e1fb-3f54-4db2-8e14-9f10d8b705cc,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,073 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 533661068, Backend time ns: 56003584
2022-09-29T22:00:39,073 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 533661068, Backend time ns: 56003584
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:533|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039073
2022-09-29T22:00:39,073 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039073
2022-09-29T22:00:39,074 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc1d3d6c-0b39-4f30-856c-a3b72fd7acaf,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fc1d3d6c-0b39-4f30-856c-a3b72fd7acaf,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 591
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,128 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535861723, Backend time ns: 55272304
2022-09-29T22:00:39,128 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535861723, Backend time ns: 55272304
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,128 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,129 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039129
2022-09-29T22:00:39,129 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039129
2022-09-29T22:00:39,130 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,201 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:71.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:61f8e8b1-fdcc-448e-95b6-63438e317708,timestamp:1664469039
2022-09-29T22:00:39,201 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:71.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:61f8e8b1-fdcc-448e-95b6-63438e317708,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 611
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,202 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537374055, Backend time ns: 73331646
2022-09-29T22:00:39,202 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537374055, Backend time ns: 73331646
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039202
2022-09-29T22:00:39,202 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039202
2022-09-29T22:00:39,203 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f9e24769-d837-4a3d-abb2-c190693f6061,timestamp:1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f9e24769-d837-4a3d-abb2-c190693f6061,timestamp:1664469039
2022-09-29T22:00:39,251 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 604
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,252 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554676771, Backend time ns: 49553649
2022-09-29T22:00:39,252 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554676771, Backend time ns: 49553649
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039252
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039252
2022-09-29T22:00:39,252 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:868ddeeb-5c06-49e0-bf60-b2a6043ae72c,timestamp:1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 589
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:868ddeeb-5c06-49e0-bf60-b2a6043ae72c,timestamp:1664469039
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,304 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536954749, Backend time ns: 52616289
2022-09-29T22:00:39,304 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536954749, Backend time ns: 52616289
2022-09-29T22:00:39,304 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:536|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039305
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039305
2022-09-29T22:00:39,305 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:00:39,369 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:63.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e7773062-8021-46de-b714-464e1df16a8b,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 601
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:63.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e7773062-8021-46de-b714-464e1df16a8b,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,370 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535501648, Backend time ns: 65172323
2022-09-29T22:00:39,370 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535501648, Backend time ns: 65172323
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039370
2022-09-29T22:00:39,370 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039370
2022-09-29T22:00:39,371 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4756e13-7e75-4a69-bde7-8d03756a11ca,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4756e13-7e75-4a69-bde7-8d03756a11ca,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 599
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546181103, Backend time ns: 52085772
2022-09-29T22:00:39,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546181103, Backend time ns: 52085772
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:546|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039422
2022-09-29T22:00:39,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039422
2022-09-29T22:00:39,423 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21bad38d-92f2-458f-bb6f-77227df6f896,timestamp:1664469039
2022-09-29T22:00:39,474 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 571
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21bad38d-92f2-458f-bb6f-77227df6f896,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,475 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519121250, Backend time ns: 52328750
2022-09-29T22:00:39,475 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519121250, Backend time ns: 52328750
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:519|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039475
2022-09-29T22:00:39,475 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039475
2022-09-29T22:00:39,476 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.04|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9ac766e-ac2a-4793-b520-30dd492602bc,timestamp:1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 575
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.08|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b9ac766e-ac2a-4793-b520-30dd492602bc,timestamp:1664469039
2022-09-29T22:00:39,541 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508629468, Backend time ns: 66685395
2022-09-29T22:00:39,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 508629468, Backend time ns: 66685395
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:508|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039542
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039542
2022-09-29T22:00:39,542 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3301fad-b465-48b9-a5c6-e06fd85a509a,timestamp:1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 598
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:73.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3301fad-b465-48b9-a5c6-e06fd85a509a,timestamp:1664469039
2022-09-29T22:00:39,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523750554, Backend time ns: 74769081
2022-09-29T22:00:39,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523750554, Backend time ns: 74769081
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039617
2022-09-29T22:00:39,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039617
2022-09-29T22:00:39,618 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3cb1a500-a384-4d7b-bcde-ec73efba8b8b,timestamp:1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:3cb1a500-a384-4d7b-bcde-ec73efba8b8b,timestamp:1664469039
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 599
2022-09-29T22:00:39,673 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 542743698, Backend time ns: 56797676
2022-09-29T22:00:39,673 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 542743698, Backend time ns: 56797676
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:542|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039674
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039674
2022-09-29T22:00:39,674 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1e1645a7-6a25-4978-b7a2-312caaa3befd,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 598
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.03|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1e1645a7-6a25-4978-b7a2-312caaa3befd,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 544255740, Backend time ns: 53170745
2022-09-29T22:00:39,727 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 544255740, Backend time ns: 53170745
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:544|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039727
2022-09-29T22:00:39,727 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039727
2022-09-29T22:00:39,728 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.75|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:87894601-6d18-45f2-9314-1af240ddcef6,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:87894601-6d18-45f2-9314-1af240ddcef6,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 577
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,780 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523854419, Backend time ns: 53140611
2022-09-29T22:00:39,780 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523854419, Backend time ns: 53140611
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039780
2022-09-29T22:00:39,780 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039780
2022-09-29T22:00:39,781 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:32b6ca04-75dd-4fa0-aeba-bb73c653c8f6,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.95|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:32b6ca04-75dd-4fa0-aeba-bb73c653c8f6,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 586
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,838 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527598274, Backend time ns: 57626782
2022-09-29T22:00:39,838 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527598274, Backend time ns: 57626782
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:527|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039838
2022-09-29T22:00:39,838 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039838
2022-09-29T22:00:39,839 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1edc7f81-668c-4eda-9a58-f4beba63341a,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.77|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1edc7f81-668c-4eda-9a58-f4beba63341a,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 586
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,891 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 532463659, Backend time ns: 53034813
2022-09-29T22:00:39,891 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 532463659, Backend time ns: 53034813
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:532|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039891
2022-09-29T22:00:39,891 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039891
2022-09-29T22:00:39,892 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9584a337-3b49-4f5a-9e47-5a9981f0d8ff,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 588
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9584a337-3b49-4f5a-9e47-5a9981f0d8ff,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:39,959 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 520492174, Backend time ns: 67605624
2022-09-29T22:00:39,959 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 520492174, Backend time ns: 67605624
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:520|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469039
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039959
2022-09-29T22:00:39,959 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469039959
2022-09-29T22:00:39,960 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469039
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,012 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21c19765-4884-4bf7-900d-72461b3a3712,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21c19765-4884-4bf7-900d-72461b3a3712,timestamp:1664469040
2022-09-29T22:00:40,013 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535954564, Backend time ns: 53767613
2022-09-29T22:00:40,013 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 535954564, Backend time ns: 53767613
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:535|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040013
2022-09-29T22:00:40,013 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040013
2022-09-29T22:00:40,014 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a020f9e-1928-4fc8-a639-40b67219ebb6,timestamp:1664469040
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a020f9e-1928-4fc8-a639-40b67219ebb6,timestamp:1664469040
2022-09-29T22:00:40,066 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537158363, Backend time ns: 53399187
2022-09-29T22:00:40,066 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537158363, Backend time ns: 53399187
2022-09-29T22:00:40,066 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040067
2022-09-29T22:00:40,067 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040067
2022-09-29T22:00:40,068 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:68523287-215d-44cf-b4bd-ad0bbbd06943,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:68523287-215d-44cf-b4bd-ad0bbbd06943,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 578
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,121 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523789094, Backend time ns: 54574267
2022-09-29T22:00:40,121 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523789094, Backend time ns: 54574267
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040121
2022-09-29T22:00:40,121 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040121
2022-09-29T22:00:40,122 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb0cd80f-e817-40af-93bc-a4dd67226f59,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb0cd80f-e817-40af-93bc-a4dd67226f59,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 557
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,175 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502795831, Backend time ns: 53560670
2022-09-29T22:00:40,175 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502795831, Backend time ns: 53560670
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:502|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040175
2022-09-29T22:00:40,175 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040175
2022-09-29T22:00:40,176 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.63|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e24e3f4-20d6-4d1f-b812-ec3b1580f195,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e24e3f4-20d6-4d1f-b812-ec3b1580f195,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,229 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500629622, Backend time ns: 54165687
2022-09-29T22:00:40,229 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 500629622, Backend time ns: 54165687
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:500|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,229 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040230
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040230
2022-09-29T22:00:40,230 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:34a5cb74-e5a8-4642-869a-38b4d75d6999,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:34a5cb74-e5a8-4642-869a-38b4d75d6999,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 557
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,285 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501651223, Backend time ns: 55642488
2022-09-29T22:00:40,285 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501651223, Backend time ns: 55642488
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040285
2022-09-29T22:00:40,285 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040285
2022-09-29T22:00:40,286 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eb222e8b-2e81-4941-b17e-4a561a62530a,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eb222e8b-2e81-4941-b17e-4a561a62530a,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 559
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,340 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504186718, Backend time ns: 55045184
2022-09-29T22:00:40,340 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 504186718, Backend time ns: 55045184
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:504|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,340 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040341
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040341
2022-09-29T22:00:40,341 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:85b0fc67-cf56-4f4d-b33d-6c2f802be3cd,timestamp:1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.52|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:85b0fc67-cf56-4f4d-b33d-6c2f802be3cd,timestamp:1664469040
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 555
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,394 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501698590, Backend time ns: 53831722
2022-09-29T22:00:40,394 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 501698590, Backend time ns: 53831722
2022-09-29T22:00:40,394 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:501|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040395
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040395
2022-09-29T22:00:40,395 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f334577-52c3-414f-a204-a8517af1624a,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f334577-52c3-414f-a204-a8517af1624a,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 560
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,452 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502452830, Backend time ns: 57783154
2022-09-29T22:00:40,452 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 502452830, Backend time ns: 57783154
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:502|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040452
2022-09-29T22:00:40,452 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040452
2022-09-29T22:00:40,453 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a0917f22-1d95-4b8f-9fe5-c48b0dc47f86,timestamp:1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 546
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a0917f22-1d95-4b8f-9fe5-c48b0dc47f86,timestamp:1664469040
2022-09-29T22:00:40,506 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492284492, Backend time ns: 53830751
2022-09-29T22:00:40,506 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492284492, Backend time ns: 53830751
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,506 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040506
2022-09-29T22:00:40,507 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040506
2022-09-29T22:00:40,508 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:753ef7ef-9907-4154-ba41-8f807cfa7898,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:753ef7ef-9907-4154-ba41-8f807cfa7898,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 549
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,563 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492611789, Backend time ns: 56789514
2022-09-29T22:00:40,563 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 492611789, Backend time ns: 56789514
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:492|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040563
2022-09-29T22:00:40,563 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040563
2022-09-29T22:00:40,564 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ae35afc-3815-495e-911d-9163f565cafd,timestamp:1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ae35afc-3815-495e-911d-9163f565cafd,timestamp:1664469040
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 559
2022-09-29T22:00:40,627 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,628 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 495491162, Backend time ns: 64046065
2022-09-29T22:00:40,628 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 495491162, Backend time ns: 64046065
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:495|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040628
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040628
2022-09-29T22:00:40,628 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fa7631f6-36c5-4e0e-83dd-e3e29b3ce3cd,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fa7631f6-36c5-4e0e-83dd-e3e29b3ce3cd,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 566
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,688 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505369484, Backend time ns: 60499603
2022-09-29T22:00:40,688 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 505369484, Backend time ns: 60499603
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:505|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040688
2022-09-29T22:00:40,688 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040688
2022-09-29T22:00:40,690 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 68
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:67.07|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2a0b5d57-567e-4684-aade-f0a77dd328a0,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 68
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:67.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2a0b5d57-567e-4684-aade-f0a77dd328a0,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 582
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,758 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 512312529, Backend time ns: 69655565
2022-09-29T22:00:40,758 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 512312529, Backend time ns: 69655565
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:512|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040758
2022-09-29T22:00:40,758 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040758
2022-09-29T22:00:40,759 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:49.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05af3ef6-d310-4fa4-a096-d75f01a71684,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:49.69|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05af3ef6-d310-4fa4-a096-d75f01a71684,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,809 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527777793, Backend time ns: 51242446
2022-09-29T22:00:40,809 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 527777793, Backend time ns: 51242446
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:527|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,809 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040810
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040810
2022-09-29T22:00:40,810 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:00:40,864 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:683a7f54-d317-402d-9092-ff39c0e2b105,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:683a7f54-d317-402d-9092-ff39c0e2b105,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 579
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523226946, Backend time ns: 55212208
2022-09-29T22:00:40,865 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523226946, Backend time ns: 55212208
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040865
2022-09-29T22:00:40,865 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040865
2022-09-29T22:00:40,866 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c2ddc397-dfcb-43c4-85ff-69cf1c2b1250,timestamp:1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.16|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c2ddc397-dfcb-43c4-85ff-69cf1c2b1250,timestamp:1664469040
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:40,931 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,931 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523417233, Backend time ns: 66616425
2022-09-29T22:00:40,931 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 523417233, Backend time ns: 66616425
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:523|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040932
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040932
2022-09-29T22:00:40,932 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5079721a-4796-44f2-b206-6209671a2480,timestamp:1664469040
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 597
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:40,992 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5079721a-4796-44f2-b206-6209671a2480,timestamp:1664469040
2022-09-29T22:00:40,993 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536146652, Backend time ns: 60905978
2022-09-29T22:00:40,993 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 536146652, Backend time ns: 60905978
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:536|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469040
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040993
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469040993
2022-09-29T22:00:40,993 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469040
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,064 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.71|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77af4fab-fe9d-404c-b05f-b5bca04bb7fb,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77af4fab-fe9d-404c-b05f-b5bca04bb7fb,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 612
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,065 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 539355724, Backend time ns: 72034328
2022-09-29T22:00:41,065 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 539355724, Backend time ns: 72034328
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:539|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041065
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041065
2022-09-29T22:00:41,065 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d0a935e-bef2-4ec5-9cf1-74bf7c714c98,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d0a935e-bef2-4ec5-9cf1-74bf7c714c98,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 611
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,118 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 557279149, Backend time ns: 53466209
2022-09-29T22:00:41,118 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 557279149, Backend time ns: 53466209
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:557|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041118
2022-09-29T22:00:41,118 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041118
2022-09-29T22:00:41,120 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,182 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,182 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:aaa008bd-233e-440a-a1f0-7f80154b944d,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:aaa008bd-233e-440a-a1f0-7f80154b944d,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 619
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,183 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 553890443, Backend time ns: 64380345
2022-09-29T22:00:41,183 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 553890443, Backend time ns: 64380345
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:553|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041183
2022-09-29T22:00:41,183 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041183
2022-09-29T22:00:41,184 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1662a337-a89a-4348-90d0-01e51258c290,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1662a337-a89a-4348-90d0-01e51258c290,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 608
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,236 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554514136, Backend time ns: 53397135
2022-09-29T22:00:41,236 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554514136, Backend time ns: 53397135
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,236 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,237 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041237
2022-09-29T22:00:41,237 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041237
2022-09-29T22:00:41,238 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,289 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:41,289 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5eb2991b-10ac-4c3a-9ef0-1e945d1122cf,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5eb2991b-10ac-4c3a-9ef0-1e945d1122cf,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 601
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546976790, Backend time ns: 53346101
2022-09-29T22:00:41,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 546976790, Backend time ns: 53346101
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:546|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041290
2022-09-29T22:00:41,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041290
2022-09-29T22:00:41,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7479b24b-a453-4ce4-b7f9-fb8fd7cce1c0,timestamp:1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7479b24b-a453-4ce4-b7f9-fb8fd7cce1c0,timestamp:1664469041
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 602
2022-09-29T22:00:41,361 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,361 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 530961435, Backend time ns: 71477374
2022-09-29T22:00:41,361 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 530961435, Backend time ns: 71477374
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:530|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041362
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041362
2022-09-29T22:00:41,362 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bc748913-eb54-43b8-921e-2490c2634fe7,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bc748913-eb54-43b8-921e-2490c2634fe7,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 612
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 551109512, Backend time ns: 60680126
2022-09-29T22:00:41,422 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 551109512, Backend time ns: 60680126
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:551|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041422
2022-09-29T22:00:41,422 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041422
2022-09-29T22:00:41,423 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9eb8972a-1d68-4d38-b81e-f3958474b083,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 632
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9eb8972a-1d68-4d38-b81e-f3958474b083,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,498 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556567568, Backend time ns: 75441992
2022-09-29T22:00:41,498 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556567568, Backend time ns: 75441992
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:556|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041498
2022-09-29T22:00:41,498 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041498
2022-09-29T22:00:41,499 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,548 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.58|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:63d7a9ee-5a4c-4c67-afcb-27d0923d16a3,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 616
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:63d7a9ee-5a4c-4c67-afcb-27d0923d16a3,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 565398167, Backend time ns: 50576528
2022-09-29T22:00:41,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 565398167, Backend time ns: 50576528
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:565|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041549
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041549
2022-09-29T22:00:41,549 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4028ea9c-b9e1-471f-ae46-253f7bb0d58c,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:69.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4028ea9c-b9e1-471f-ae46-253f7bb0d58c,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 627
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 555134781, Backend time ns: 71198078
2022-09-29T22:00:41,620 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 555134781, Backend time ns: 71198078
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:555|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041620
2022-09-29T22:00:41,620 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041620
2022-09-29T22:00:41,621 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:61.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b782014-42a0-449b-b6cb-c08ad679240d,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:61.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b782014-42a0-449b-b6cb-c08ad679240d,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 617
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,683 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554300266, Backend time ns: 62763393
2022-09-29T22:00:41,683 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554300266, Backend time ns: 62763393
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041683
2022-09-29T22:00:41,683 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041683
2022-09-29T22:00:41,684 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:41,740 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f93aa80-3eb9-4cb3-88c7-ca1b64494ac4,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f93aa80-3eb9-4cb3-88c7-ca1b64494ac4,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 622
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,741 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563673556, Backend time ns: 57707611
2022-09-29T22:00:41,741 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563673556, Backend time ns: 57707611
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:563|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041741
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041741
2022-09-29T22:00:41,741 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:70.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:41bd749b-ae2b-47bc-8ca6-bee6d618182c,timestamp:1664469041
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59066 "POST /predictions/benchmark HTTP/1.0" 200 628
2022-09-29T22:00:41,812 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:41bd749b-ae2b-47bc-8ca6-bee6d618182c,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556880524, Backend time ns: 71770531
2022-09-29T22:00:41,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 556880524, Backend time ns: 71770531
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:556|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041813
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041813
2022-09-29T22:00:41,813 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 49
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:48.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eeefca7c-d165-40cf-a020-da743dc8b9c9,timestamp:1664469041
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58988 "POST /predictions/benchmark HTTP/1.0" 200 625
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,862 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:48.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eeefca7c-d165-40cf-a020-da743dc8b9c9,timestamp:1664469041
2022-09-29T22:00:41,862 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 575337827, Backend time ns: 49742678
2022-09-29T22:00:41,862 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 575337827, Backend time ns: 49742678
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:575|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041863
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041863
2022-09-29T22:00:41,863 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b95a7065-3685-428a-b5a7-8cab09c0bcc7,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58990 "POST /predictions/benchmark HTTP/1.0" 200 635
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,926 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 571571363, Backend time ns: 63663022
2022-09-29T22:00:41,926 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 571571363, Backend time ns: 63663022
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b95a7065-3685-428a-b5a7-8cab09c0bcc7,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:571|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041926
2022-09-29T22:00:41,926 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041926
2022-09-29T22:00:41,927 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 51
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:50.29|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5340de69-f4d5-41aa-9bee-3bde6833ff00,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58992 "POST /predictions/benchmark HTTP/1.0" 200 616
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:50.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5340de69-f4d5-41aa-9bee-3bde6833ff00,timestamp:1664469041
2022-09-29T22:00:41,978 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563864296, Backend time ns: 51385280
2022-09-29T22:00:41,978 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 563864296, Backend time ns: 51385280
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:563|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469041
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041978
2022-09-29T22:00:41,978 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469041978
2022-09-29T22:00:41,979 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469041
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.99|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:74d7b011-de35-47f4-9b72-5c77dd92229b,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59008 "POST /predictions/benchmark HTTP/1.0" 200 608
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:74d7b011-de35-47f4-9b72-5c77dd92229b,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,031 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554746364, Backend time ns: 53135430
2022-09-29T22:00:42,031 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554746364, Backend time ns: 53135430
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042031
2022-09-29T22:00:42,031 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042031
2022-09-29T22:00:42,032 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b54bdf48-f129-498b-b7b0-a357802bcb93,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b54bdf48-f129-498b-b7b0-a357802bcb93,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59024 "POST /predictions/benchmark HTTP/1.0" 200 588
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 531894832, Backend time ns: 55646645
2022-09-29T22:00:42,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 531894832, Backend time ns: 55646645
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:531|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042087
2022-09-29T22:00:42,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042087
2022-09-29T22:00:42,088 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21154f13-0902-4d86-8dc2-7a80f890a4c1,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:58996 "POST /predictions/benchmark HTTP/1.0" 200 590
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.84|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:21154f13-0902-4d86-8dc2-7a80f890a4c1,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,140 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537273904, Backend time ns: 52911870
2022-09-29T22:00:42,140 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 537273904, Backend time ns: 52911870
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:537|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042140
2022-09-29T22:00:42,140 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042140
2022-09-29T22:00:42,141 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dde6b1a3-6cf2-48b2-a69d-58232f6b5757,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.83|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dde6b1a3-6cf2-48b2-a69d-58232f6b5757,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59014 "POST /predictions/benchmark HTTP/1.0" 200 576
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519141247, Backend time ns: 57019860
2022-09-29T22:00:42,197 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 519141247, Backend time ns: 57019860
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:519|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042197
2022-09-29T22:00:42,197 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042197
2022-09-29T22:00:42,198 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.55|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05c52c23-973c-4693-9527-0a7cc6ad4f04,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.59|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:05c52c23-973c-4693-9527-0a7cc6ad4f04,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59040 "POST /predictions/benchmark HTTP/1.0" 200 571
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,255 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513571877, Backend time ns: 57758451
2022-09-29T22:00:42,255 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513571877, Backend time ns: 57758451
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:513|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042255
2022-09-29T22:00:42,255 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664469042255
2022-09-29T22:00:42,256 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.78|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:de635535-f211-4c67-93aa-6f0406093dd7,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:de635535-f211-4c67-93aa-6f0406093dd7,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:59056 "POST /predictions/benchmark HTTP/1.0" 200 567
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:00:42,308 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513615334, Backend time ns: 52831536
2022-09-29T22:00:42,308 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513615334, Backend time ns: 52831536
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:513|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469042
2022-09-29T22:00:42,312 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:00:42,312 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:00:42,313 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,313 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,313 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:00:42,313 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Frontend disconnected.
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:00:42,314 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:00:42,333 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:00:42,333 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:00:42,340 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,340 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:00:42,344 [INFO ] epollEventLoopGroup-3-23 ACCESS_LOG - /127.0.0.1:58116 "DELETE /models/benchmark HTTP/1.1" 200 32
2022-09-29T22:00:42,344 [INFO ] epollEventLoopGroup-3-23 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664469022
2022-09-29T22:28:30,200 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:28:30,200 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-09-29T22:28:30,330 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:28:30,330 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.6.0
TS Home: /home/sasikumar/.local/lib/python3.10/site-packages
Current directory: /home/sasikumar/personal/torchserve/serve/benchmarks
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 4096 M
Python executable: /usr/bin/python3
Config file: /tmp/benchmark/conf/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: /tmp/model_store
Initial Models: N/A
Log dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Metrics dir: /home/sasikumar/personal/torchserve/serve/benchmarks/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: True
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /tmp/wf_store
Model config: N/A
2022-09-29T22:28:30,351 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:28:30,351 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-09-29T22:28:30,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:28:30,395 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-09-29T22:28:30,452 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:28:30,452 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-09-29T22:28:30,453 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:28:30,453 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-09-29T22:28:30,454 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:28:30,454 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2022-09-29T22:28:30,454 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:28:30,454 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-09-29T22:28:30,455 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:28:30,455 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-09-29T22:28:30,844 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,847 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:163.2534294128418|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,849 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:58.01225280761719|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,852 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:26.2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,862 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2942.28515625|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,864 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3827.6640625|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:30,865 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:62.4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470710
2022-09-29T22:28:32,605 [INFO ] pool-2-thread-1 ACCESS_LOG - /127.0.0.1:42086 "GET /ping HTTP/1.1" 200 6
2022-09-29T22:28:32,605 [INFO ] pool-2-thread-1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:42,539 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:28:42,539 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model benchmark
2022-09-29T22:28:42,540 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:28:42,540 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model benchmark
2022-09-29T22:28:42,540 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:28:42,540 [INFO ] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - Model benchmark loaded.
2022-09-29T22:28:42,541 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:28:42,541 [DEBUG] epollEventLoopGroup-3-2 org.pytorch.serve.wlm.ModelManager - updateModel: benchmark, count: 1
2022-09-29T22:28:42,545 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:28:42,545 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/sasikumar/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-09-29T22:28:43,164 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-09-29T22:28:43,165 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - [PID]22286
2022-09-29T22:28:43,165 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Torch worker started.
2022-09-29T22:28:43,165 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Python runtime: 3.10.6
2022-09-29T22:28:43,165 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:28:43,165 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change null -> WORKER_STARTED
2022-09-29T22:28:43,168 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:28:43,168 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-09-29T22:28:43,172 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-09-29T22:28:43,174 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470723174
2022-09-29T22:28:43,174 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470723174
2022-09-29T22:28:43,182 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - model_name: benchmark, batchSize: 1
2022-09-29T22:28:44,214 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1032
2022-09-29T22:28:44,214 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1032
2022-09-29T22:28:44,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:28:44,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-09-29T22:28:44,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - W-9000-benchmark_1.0.ms:1671|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,216 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:37656 "POST /models?model_name=benchmark&url=https%3A%2F%2Ftorchserve.pytorch.org%2Fmar_files%2Fresnet-18.mar&batch_delay=200&batch_size=1&initial_workers=1&synchronous=true HTTP/1.1" 200 11594
2022-09-29T22:28:44,216 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,265 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724265
2022-09-29T22:28:44,265 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724265
2022-09-29T22:28:44,269 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:44,368 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:96.88|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:48b69722-2085-4278-81aa-262d2f2c4464,timestamp:1664470724
2022-09-29T22:28:44,369 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:44,369 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:96.93|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:48b69722-2085-4278-81aa-262d2f2c4464,timestamp:1664470724
2022-09-29T22:28:44,369 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46572 "POST /predictions/benchmark HTTP/1.0" 200 118
2022-09-29T22:28:44,371 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,371 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 153922, Backend time ns: 106492231
2022-09-29T22:28:44,371 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 153922, Backend time ns: 106492231
2022-09-29T22:28:44,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,372 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,377 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724377
2022-09-29T22:28:44,377 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724377
2022-09-29T22:28:44,383 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 170
2022-09-29T22:28:44,548 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:164.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4f2e6f0d-e4f5-4a30-96a7-352bd216808a,timestamp:1664470724
2022-09-29T22:28:44,548 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 170
2022-09-29T22:28:44,549 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46572 "POST /predictions/benchmark HTTP/1.0" 200 172
2022-09-29T22:28:44,549 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 76002, Backend time ns: 172183260
2022-09-29T22:28:44,549 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 76002, Backend time ns: 172183260
2022-09-29T22:28:44,550 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,550 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,551 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724551
2022-09-29T22:28:44,550 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:164.41|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4f2e6f0d-e4f5-4a30-96a7-352bd216808a,timestamp:1664470724
2022-09-29T22:28:44,551 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724551
2022-09-29T22:28:44,552 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:62.41|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e2c9bec8-a425-48fe-82f2-72d6ce9e3ed3,timestamp:1664470724
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:62.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e2c9bec8-a425-48fe-82f2-72d6ce9e3ed3,timestamp:1664470724
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46626 "POST /predictions/benchmark HTTP/1.0" 200 187
2022-09-29T22:28:44,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,617 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 121592531, Backend time ns: 65992477
2022-09-29T22:28:44,617 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 121592531, Backend time ns: 65992477
2022-09-29T22:28:44,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:121|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,617 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724617
2022-09-29T22:28:44,617 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724617
2022-09-29T22:28:44,619 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,671 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:44,671 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5fb8353e-f7fd-4f1a-ab7b-2f86b236da75,timestamp:1664470724
2022-09-29T22:28:44,671 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:44,672 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.93|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5fb8353e-f7fd-4f1a-ab7b-2f86b236da75,timestamp:1664470724
2022-09-29T22:28:44,672 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46604 "POST /predictions/benchmark HTTP/1.0" 200 212
2022-09-29T22:28:44,672 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,672 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 156450152, Backend time ns: 54842713
2022-09-29T22:28:44,672 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 156450152, Backend time ns: 54842713
2022-09-29T22:28:44,672 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:156|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,673 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724673
2022-09-29T22:28:44,673 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724673
2022-09-29T22:28:44,674 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,749 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:44,749 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46638 "POST /predictions/benchmark HTTP/1.0" 200 305
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:67cccef2-c8ea-41b2-bb01-92d9a7969a19,timestamp:1664470724
2022-09-29T22:28:44,750 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 211951776, Backend time ns: 76838978
2022-09-29T22:28:44,750 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 211951776, Backend time ns: 76838978
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:67cccef2-c8ea-41b2-bb01-92d9a7969a19,timestamp:1664470724
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:211|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724750
2022-09-29T22:28:44,750 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724750
2022-09-29T22:28:44,754 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,814 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:28:44,814 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:28:44,814 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46656 "POST /predictions/benchmark HTTP/1.0" 200 337
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.6|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:110fca71-6283-44dc-b7e0-72f6c5ddb1e8,timestamp:1664470724
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,815 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 273332241, Backend time ns: 64385276
2022-09-29T22:28:44,815 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 273332241, Backend time ns: 64385276
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:273|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.77|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:110fca71-6283-44dc-b7e0-72f6c5ddb1e8,timestamp:1664470724
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724815
2022-09-29T22:28:44,815 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724815
2022-09-29T22:28:44,817 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,878 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:28:44,878 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:28:44,878 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:60.65|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d76bda90-0d65-42b2-8a88-238c62d43ea8,timestamp:1664470724
2022-09-29T22:28:44,879 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:60.69|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d76bda90-0d65-42b2-8a88-238c62d43ea8,timestamp:1664470724
2022-09-29T22:28:44,879 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46588 "POST /predictions/benchmark HTTP/1.0" 200 414
2022-09-29T22:28:44,879 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,879 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 337616186, Backend time ns: 63574937
2022-09-29T22:28:44,879 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 337616186, Backend time ns: 63574937
2022-09-29T22:28:44,879 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:337|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,879 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724880
2022-09-29T22:28:44,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724880
2022-09-29T22:28:44,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:44,937 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:44,937 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7f8b6a3b-9b81-4fb7-993e-a8c7f79fb04a,timestamp:1664470724
2022-09-29T22:28:44,937 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:44,938 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7f8b6a3b-9b81-4fb7-993e-a8c7f79fb04a,timestamp:1664470724
2022-09-29T22:28:44,938 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46616 "POST /predictions/benchmark HTTP/1.0" 200 457
2022-09-29T22:28:44,938 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:44,938 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 398250500, Backend time ns: 58743789
2022-09-29T22:28:44,938 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 398250500, Backend time ns: 58743789
2022-09-29T22:28:44,938 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:398|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,939 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470724
2022-09-29T22:28:44,939 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724939
2022-09-29T22:28:44,939 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470724939
2022-09-29T22:28:44,940 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470724
2022-09-29T22:28:45,008 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:28:45,008 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 69
2022-09-29T22:28:45,008 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:67.72|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:afc82c3a-c72f-45ee-864c-99ad3f7a9002,timestamp:1664470725
2022-09-29T22:28:45,009 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46652 "POST /predictions/benchmark HTTP/1.0" 200 533
2022-09-29T22:28:45,009 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:67.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:afc82c3a-c72f-45ee-864c-99ad3f7a9002,timestamp:1664470725
2022-09-29T22:28:45,009 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,009 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 456929534, Backend time ns: 70389559
2022-09-29T22:28:45,009 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 456929534, Backend time ns: 70389559
2022-09-29T22:28:45,009 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:456|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,010 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,010 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725010
2022-09-29T22:28:45,010 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725010
2022-09-29T22:28:45,011 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 121
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:117.94|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a813ad43-6b72-45e2-b6da-1cd961c2be8c,timestamp:1664470725
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46618 "POST /predictions/benchmark HTTP/1.0" 200 635
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,131 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:118.05|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a813ad43-6b72-45e2-b6da-1cd961c2be8c,timestamp:1664470725
2022-09-29T22:28:45,131 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513132642, Backend time ns: 121597157
2022-09-29T22:28:45,131 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 513132642, Backend time ns: 121597157
2022-09-29T22:28:45,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:513|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,132 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,152 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725152
2022-09-29T22:28:45,152 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725152
2022-09-29T22:28:45,154 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0060fdd-9afa-45be-8042-e5fc7619023b,timestamp:1664470725
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 59
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 69061, Backend time ns: 58955957
2022-09-29T22:28:45,211 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 69061, Backend time ns: 58955957
2022-09-29T22:28:45,211 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,212 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,212 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c0060fdd-9afa-45be-8042-e5fc7619023b,timestamp:1664470725
2022-09-29T22:28:45,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725215
2022-09-29T22:28:45,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725215
2022-09-29T22:28:45,216 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,306 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:89.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eacd884f-7bcc-4699-8974-b7faa9ecc93a,timestamp:1664470725
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 90
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 90
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:89.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:eacd884f-7bcc-4699-8974-b7faa9ecc93a,timestamp:1664470725
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 92
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,307 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180637, Backend time ns: 92113376
2022-09-29T22:28:45,307 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 180637, Backend time ns: 92113376
2022-09-29T22:28:45,307 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,308 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725308
2022-09-29T22:28:45,308 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725308
2022-09-29T22:28:45,309 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,380 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dbf21eba-b7fd-4881-9fea-90dfef0c5ea9,timestamp:1664470725
2022-09-29T22:28:45,380 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:45,380 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 158
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:69.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dbf21eba-b7fd-4881-9fea-90dfef0c5ea9,timestamp:1664470725
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,381 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84384950, Backend time ns: 73254226
2022-09-29T22:28:45,381 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 84384950, Backend time ns: 73254226
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:84|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725381
2022-09-29T22:28:45,381 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725381
2022-09-29T22:28:45,385 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,464 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:28:45,464 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 82
2022-09-29T22:28:45,465 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 236
2022-09-29T22:28:45,465 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,465 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 152237526, Backend time ns: 83642661
2022-09-29T22:28:45,465 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 152237526, Backend time ns: 83642661
2022-09-29T22:28:45,465 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:152|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,465 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,466 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725466
2022-09-29T22:28:45,466 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725466
2022-09-29T22:28:45,466 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:77.58|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7def6f3f-2fd3-4e4e-9396-0e124f4c209a,timestamp:1664470725
2022-09-29T22:28:45,467 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:77.63|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7def6f3f-2fd3-4e4e-9396-0e124f4c209a,timestamp:1664470725
2022-09-29T22:28:45,468 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:45,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.64|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e0cb9f92-bfda-4049-a149-63ad50d19d48,timestamp:1664470725
2022-09-29T22:28:45,541 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:45,541 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.72|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e0cb9f92-bfda-4049-a149-63ad50d19d48,timestamp:1664470725
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 308
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 232765753, Backend time ns: 76102233
2022-09-29T22:28:45,542 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 232765753, Backend time ns: 76102233
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:232|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725542
2022-09-29T22:28:45,542 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725542
2022-09-29T22:28:45,543 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 395
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:87.19|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ac7e9f9-efa2-4771-9157-5ce202a2ba93,timestamp:1664470725
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,632 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 304643451, Backend time ns: 89899892
2022-09-29T22:28:45,632 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 304643451, Backend time ns: 89899892
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:304|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,632 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,633 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725633
2022-09-29T22:28:45,633 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725633
2022-09-29T22:28:45,637 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:87.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ac7e9f9-efa2-4771-9157-5ce202a2ba93,timestamp:1664470725
2022-09-29T22:28:45,639 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:68.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:509d5975-adeb-4277-b6fb-bf9bbb4d40c9,timestamp:1664470725
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 463
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:68.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:509d5975-adeb-4277-b6fb-bf9bbb4d40c9,timestamp:1664470725
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,709 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 385910935, Backend time ns: 76653597
2022-09-29T22:28:45,709 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 385910935, Backend time ns: 76653597
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:385|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,709 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725710
2022-09-29T22:28:45,710 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725710
2022-09-29T22:28:45,714 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c3b4c048-2a78-44b6-905c-d52a23d25cca,timestamp:1664470725
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 514
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.93|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c3b4c048-2a78-44b6-905c-d52a23d25cca,timestamp:1664470725
2022-09-29T22:28:45,771 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,772 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 452178310, Backend time ns: 62132760
2022-09-29T22:28:45,772 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 452178310, Backend time ns: 62132760
2022-09-29T22:28:45,772 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:452|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,772 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,772 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725772
2022-09-29T22:28:45,772 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725772
2022-09-29T22:28:45,774 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:60.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ce56f5a0-421e-4a1d-8388-0eb6b2421b57,timestamp:1664470725
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:60.71|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ce56f5a0-421e-4a1d-8388-0eb6b2421b57,timestamp:1664470725
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 575
2022-09-29T22:28:45,835 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,835 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 511412155, Backend time ns: 63386592
2022-09-29T22:28:45,835 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 511412155, Backend time ns: 63386592
2022-09-29T22:28:45,836 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:511|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,836 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,836 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725836
2022-09-29T22:28:45,836 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725836
2022-09-29T22:28:45,837 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.2|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e2fe0f12-e7cc-4c65-ae4f-8fc42b96926b,timestamp:1664470725
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.24|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:e2fe0f12-e7cc-4c65-ae4f-8fc42b96926b,timestamp:1664470725
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 626
2022-09-29T22:28:45,893 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,894 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 568777553, Backend time ns: 57848263
2022-09-29T22:28:45,894 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 568777553, Backend time ns: 57848263
2022-09-29T22:28:45,894 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:568|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,894 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,894 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725894
2022-09-29T22:28:45,894 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725894
2022-09-29T22:28:45,895 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3604b84-93c4-4b29-b240-bebc6a186b92,timestamp:1664470725
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 702
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:45,969 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b3604b84-93c4-4b29-b240-bebc6a186b92,timestamp:1664470725
2022-09-29T22:28:45,969 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 626874704, Backend time ns: 75298369
2022-09-29T22:28:45,969 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 626874704, Backend time ns: 75298369
2022-09-29T22:28:45,970 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:626|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,970 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470725
2022-09-29T22:28:45,970 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725970
2022-09-29T22:28:45,970 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470725970
2022-09-29T22:28:45,971 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470725
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:67.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0e24adaf-a5b2-4687-b02b-520d734f9c4c,timestamp:1664470726
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:67.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0e24adaf-a5b2-4687-b02b-520d734f9c4c,timestamp:1664470726
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 730
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,040 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 659296175, Backend time ns: 70550212
2022-09-29T22:28:46,040 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 659296175, Backend time ns: 70550212
2022-09-29T22:28:46,040 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:659|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,041 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,041 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726041
2022-09-29T22:28:46,041 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726041
2022-09-29T22:28:46,043 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:28:46,109 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:181c60f7-d83d-4b2b-898a-ac964931b65b,timestamp:1664470726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:181c60f7-d83d-4b2b-898a-ac964931b65b,timestamp:1664470726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,110 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 656108909, Backend time ns: 69366164
2022-09-29T22:28:46,110 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 656108909, Backend time ns: 69366164
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:656|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726110
2022-09-29T22:28:46,110 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726110
2022-09-29T22:28:46,112 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:46,187 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:46,187 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f3ee9b6e-6a79-4804-9b00-1a1e74ed3a97,timestamp:1664470726
2022-09-29T22:28:46,188 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 720
2022-09-29T22:28:46,189 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,189 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f3ee9b6e-6a79-4804-9b00-1a1e74ed3a97,timestamp:1664470726
2022-09-29T22:28:46,189 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 642134939, Backend time ns: 78736733
2022-09-29T22:28:46,189 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 642134939, Backend time ns: 78736733
2022-09-29T22:28:46,190 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:642|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,190 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,190 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726190
2022-09-29T22:28:46,190 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726190
2022-09-29T22:28:46,193 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,270 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:28:46,270 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:77.29|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c8148a14-84f6-465d-9243-a49542692791,timestamp:1664470726
2022-09-29T22:28:46,270 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:77.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c8148a14-84f6-465d-9243-a49542692791,timestamp:1664470726
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 727
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,271 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 645645479, Backend time ns: 80637532
2022-09-29T22:28:46,271 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 645645479, Backend time ns: 80637532
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:645|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726271
2022-09-29T22:28:46,271 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726271
2022-09-29T22:28:46,272 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:66.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d54b053f-2692-4df1-ab75-3a5adb5efa33,timestamp:1664470726
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 698
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:66.38|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d54b053f-2692-4df1-ab75-3a5adb5efa33,timestamp:1664470726
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,339 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 630116752, Backend time ns: 67944054
2022-09-29T22:28:46,339 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 630116752, Backend time ns: 67944054
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:630|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726339
2022-09-29T22:28:46,339 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726339
2022-09-29T22:28:46,340 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,417 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:46,417 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:46,418 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 705
2022-09-29T22:28:46,418 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,418 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 627464545, Backend time ns: 78987310
2022-09-29T22:28:46,418 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 627464545, Backend time ns: 78987310
2022-09-29T22:28:46,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:627|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,419 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,419 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726419
2022-09-29T22:28:46,419 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726419
2022-09-29T22:28:46,424 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ec65e670-3c67-4e72-ad83-eaf6c282df64,timestamp:1664470726
2022-09-29T22:28:46,424 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ec65e670-3c67-4e72-ad83-eaf6c282df64,timestamp:1664470726
2022-09-29T22:28:46,424 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,507 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:82.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:06107ff2-c6ab-4ed7-b85f-e74247cc9bac,timestamp:1664470726
2022-09-29T22:28:46,507 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:82.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:06107ff2-c6ab-4ed7-b85f-e74247cc9bac,timestamp:1664470726
2022-09-29T22:28:46,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 86
2022-09-29T22:28:46,508 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 86
2022-09-29T22:28:46,508 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 734
2022-09-29T22:28:46,508 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,516 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 644490902, Backend time ns: 96534898
2022-09-29T22:28:46,516 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 644490902, Backend time ns: 96534898
2022-09-29T22:28:46,517 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:644|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,517 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:12|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,518 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726518
2022-09-29T22:28:46,518 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726518
2022-09-29T22:28:46,523 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,608 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:46,608 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:46,608 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:88.18|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dad10dc6-9add-41dd-80ca-809543b375ee,timestamp:1664470726
2022-09-29T22:28:46,608 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:88.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:dad10dc6-9add-41dd-80ca-809543b375ee,timestamp:1664470726
2022-09-29T22:28:46,608 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 771
2022-09-29T22:28:46,609 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,609 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 680753482, Backend time ns: 90565989
2022-09-29T22:28:46,609 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 680753482, Backend time ns: 90565989
2022-09-29T22:28:46,609 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:680|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,609 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,609 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726609
2022-09-29T22:28:46,609 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726609
2022-09-29T22:28:46,610 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.87|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d4dd20a-03ce-4ed6-ad09-6ea9e95afe8d,timestamp:1664470726
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.91|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4d4dd20a-03ce-4ed6-ad09-6ea9e95afe8d,timestamp:1664470726
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 768
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,663 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 713439708, Backend time ns: 54042939
2022-09-29T22:28:46,663 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 713439708, Backend time ns: 54042939
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:713|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726663
2022-09-29T22:28:46,663 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726663
2022-09-29T22:28:46,664 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77e2f295-f72b-4266-97ec-153fab44ff77,timestamp:1664470726
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 742
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:77e2f295-f72b-4266-97ec-153fab44ff77,timestamp:1664470726
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,717 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 688154538, Backend time ns: 53936159
2022-09-29T22:28:46,717 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 688154538, Backend time ns: 53936159
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:688|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,717 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,718 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726718
2022-09-29T22:28:46,718 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726718
2022-09-29T22:28:46,718 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:28:46,770 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4b44410c-9dba-4f7a-a2d2-cca83a1484c5,timestamp:1664470726
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 729
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,771 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 674782984, Backend time ns: 53404552
2022-09-29T22:28:46,771 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 674782984, Backend time ns: 53404552
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:674|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726771
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726771
2022-09-29T22:28:46,771 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.57|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4b44410c-9dba-4f7a-a2d2-cca83a1484c5,timestamp:1664470726
2022-09-29T22:28:46,772 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,826 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:46,826 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:46,826 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.65|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a4807349-52ff-4d9d-b300-de1d7dcddf62,timestamp:1664470726
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 715
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.68|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a4807349-52ff-4d9d-b300-de1d7dcddf62,timestamp:1664470726
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,827 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 659068423, Backend time ns: 55671358
2022-09-29T22:28:46,827 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 659068423, Backend time ns: 55671358
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:659|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726827
2022-09-29T22:28:46,827 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726827
2022-09-29T22:28:46,828 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a7375b04-451d-482b-a5be-a092e197d0c8,timestamp:1664470726
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.84|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a7375b04-451d-482b-a5be-a092e197d0c8,timestamp:1664470726
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 689
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,881 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 634957671, Backend time ns: 53922070
2022-09-29T22:28:46,881 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 634957671, Backend time ns: 53922070
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:634|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726881
2022-09-29T22:28:46,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726881
2022-09-29T22:28:46,882 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 53
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c452f492-136b-491f-aa9d-69735f91a3a2,timestamp:1664470726
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c452f492-136b-491f-aa9d-69735f91a3a2,timestamp:1664470726
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 660
2022-09-29T22:28:46,935 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,936 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 606489911, Backend time ns: 54242121
2022-09-29T22:28:46,936 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 606489911, Backend time ns: 54242121
2022-09-29T22:28:46,936 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:606|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,936 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726936
2022-09-29T22:28:46,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726936
2022-09-29T22:28:46,937 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:52.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0bffd817-f363-4f4f-a9f9-5e58eed7c55f,timestamp:1664470726
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 649
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:46,990 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:52.47|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0bffd817-f363-4f4f-a9f9-5e58eed7c55f,timestamp:1664470726
2022-09-29T22:28:46,991 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 594908953, Backend time ns: 54554948
2022-09-29T22:28:46,991 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 594908953, Backend time ns: 54554948
2022-09-29T22:28:46,991 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:594|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,991 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470726
2022-09-29T22:28:46,991 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726991
2022-09-29T22:28:46,991 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470726991
2022-09-29T22:28:46,993 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470726
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 642
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,064 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 568509020, Backend time ns: 73524569
2022-09-29T22:28:47,064 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 568509020, Backend time ns: 73524569
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:568|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,064 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727065
2022-09-29T22:28:47,065 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727065
2022-09-29T22:28:47,065 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f888bb0-464e-42dc-bcff-b2177a0b8272,timestamp:1664470727
2022-09-29T22:28:47,066 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:69.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:9f888bb0-464e-42dc-bcff-b2177a0b8272,timestamp:1664470727
2022-09-29T22:28:47,068 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,150 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:47,150 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:47,150 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 641
2022-09-29T22:28:47,150 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,150 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554950788, Backend time ns: 85719444
2022-09-29T22:28:47,150 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 554950788, Backend time ns: 85719444
2022-09-29T22:28:47,150 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:554|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,151 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,151 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727151
2022-09-29T22:28:47,151 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727151
2022-09-29T22:28:47,152 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:81.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:808be7b7-f669-40f7-9822-f468b419c53f,timestamp:1664470727
2022-09-29T22:28:47,154 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:81.28|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:808be7b7-f669-40f7-9822-f468b419c53f,timestamp:1664470727
2022-09-29T22:28:47,155 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:47,229 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:47,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c74bd065-0ce1-4998-88a0-7ca19ce036fa,timestamp:1664470727
2022-09-29T22:28:47,229 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c74bd065-0ce1-4998-88a0-7ca19ce036fa,timestamp:1664470727
2022-09-29T22:28:47,229 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 619
2022-09-29T22:28:47,230 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,230 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 540452069, Backend time ns: 79318789
2022-09-29T22:28:47,230 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 540452069, Backend time ns: 79318789
2022-09-29T22:28:47,230 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:540|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,231 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,231 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727231
2022-09-29T22:28:47,231 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727231
2022-09-29T22:28:47,232 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:81.75|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d6bd7e0c-8657-4827-9ccb-ea4614c9b390,timestamp:1664470727
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 651
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,315 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 566272847, Backend time ns: 84773015
2022-09-29T22:28:47,315 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 566272847, Backend time ns: 84773015
2022-09-29T22:28:47,315 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:81.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d6bd7e0c-8657-4827-9ccb-ea4614c9b390,timestamp:1664470727
2022-09-29T22:28:47,316 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:566|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,316 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,316 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727316
2022-09-29T22:28:47,316 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727316
2022-09-29T22:28:47,318 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:47,400 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:81.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20aaa01c-64c3-4974-aea0-b90c19ccbac1,timestamp:1664470727
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 683
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,401 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 597069978, Backend time ns: 85224142
2022-09-29T22:28:47,401 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 597069978, Backend time ns: 85224142
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:597|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727401
2022-09-29T22:28:47,401 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727401
2022-09-29T22:28:47,403 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:81.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:20aaa01c-64c3-4974-aea0-b90c19ccbac1,timestamp:1664470727
2022-09-29T22:28:47,403 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,479 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f98c6e09-f96a-4404-b80c-21f7ca33aa0d,timestamp:1664470727
2022-09-29T22:28:47,479 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:47,479 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 708
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,480 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 628801713, Backend time ns: 78564680
2022-09-29T22:28:47,480 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 628801713, Backend time ns: 78564680
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:628|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727480
2022-09-29T22:28:47,480 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727480
2022-09-29T22:28:47,482 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f98c6e09-f96a-4404-b80c-21f7ca33aa0d,timestamp:1664470727
2022-09-29T22:28:47,483 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:77.28|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f41970a1-4206-4924-b12f-05990a7b6623,timestamp:1664470727
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:77.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f41970a1-4206-4924-b12f-05990a7b6623,timestamp:1664470727
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 733
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,561 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 651791088, Backend time ns: 81184099
2022-09-29T22:28:47,561 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 651791088, Backend time ns: 81184099
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:651|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,561 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,562 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727562
2022-09-29T22:28:47,562 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727562
2022-09-29T22:28:47,563 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 79
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:78.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8708e76b-557b-4db4-a989-95f33236417f,timestamp:1664470727
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 761
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:78.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8708e76b-557b-4db4-a989-95f33236417f,timestamp:1664470727
2022-09-29T22:28:47,643 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 678954604, Backend time ns: 81559368
2022-09-29T22:28:47,643 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 678954604, Backend time ns: 81559368
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:678|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727643
2022-09-29T22:28:47,643 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727643
2022-09-29T22:28:47,644 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,711 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:47,711 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:47,711 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:66.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d19a3cc3-e528-4341-ab36-1038360c6472,timestamp:1664470727
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:66.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d19a3cc3-e528-4341-ab36-1038360c6472,timestamp:1664470727
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 775
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,712 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 706325847, Backend time ns: 68494785
2022-09-29T22:28:47,712 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 706325847, Backend time ns: 68494785
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:706|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727712
2022-09-29T22:28:47,712 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727712
2022-09-29T22:28:47,713 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 99
2022-09-29T22:28:47,812 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 99
2022-09-29T22:28:47,812 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:97.9|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15e27bf3-cc07-466b-95c7-e14c1597307a,timestamp:1664470727
2022-09-29T22:28:47,812 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 820
2022-09-29T22:28:47,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,813 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:97.94|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15e27bf3-cc07-466b-95c7-e14c1597307a,timestamp:1664470727
2022-09-29T22:28:47,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 719665638, Backend time ns: 101116382
2022-09-29T22:28:47,813 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 719665638, Backend time ns: 101116382
2022-09-29T22:28:47,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:719|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,813 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,814 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727814
2022-09-29T22:28:47,814 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727814
2022-09-29T22:28:47,816 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:47,916 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 101
2022-09-29T22:28:47,916 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 101
2022-09-29T22:28:47,916 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 851
2022-09-29T22:28:47,916 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:47,917 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 747595328, Backend time ns: 103055263
2022-09-29T22:28:47,917 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 747595328, Backend time ns: 103055263
2022-09-29T22:28:47,917 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:747|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,917 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470727
2022-09-29T22:28:47,917 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727917
2022-09-29T22:28:47,917 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470727917
2022-09-29T22:28:47,917 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:99.56|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15131985-4f8e-4f57-a649-de5e2d2027ed,timestamp:1664470727
2022-09-29T22:28:47,919 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:99.61|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15131985-4f8e-4f57-a649-de5e2d2027ed,timestamp:1664470727
2022-09-29T22:28:47,920 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470727
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 111
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:107.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:31eeadca-083f-412e-ba3b-c60fd4934ed4,timestamp:1664470728
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 111
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:107.54|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:31eeadca-083f-412e-ba3b-c60fd4934ed4,timestamp:1664470728
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 875
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 763527865, Backend time ns: 111510228
2022-09-29T22:28:48,028 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 763527865, Backend time ns: 111510228
2022-09-29T22:28:48,028 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:763|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,029 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,029 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728029
2022-09-29T22:28:48,029 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728029
2022-09-29T22:28:48,036 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 111
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 111
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:106.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d52c9b92-a908-4740-ba70-2399a7222975,timestamp:1664470728
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:106.49|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d52c9b92-a908-4740-ba70-2399a7222975,timestamp:1664470728
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 910
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,143 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 795005251, Backend time ns: 114394559
2022-09-29T22:28:48,143 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 795005251, Backend time ns: 114394559
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:795|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728143
2022-09-29T22:28:48,143 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728143
2022-09-29T22:28:48,145 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,253 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 108
2022-09-29T22:28:48,253 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 108
2022-09-29T22:28:48,253 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 936
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:105.94|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4dc633c9-ff26-4f74-988f-abd3cb586355,timestamp:1664470728
2022-09-29T22:28:48,254 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 826008603, Backend time ns: 110822767
2022-09-29T22:28:48,254 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 826008603, Backend time ns: 110822767
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:826|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:106.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4dc633c9-ff26-4f74-988f-abd3cb586355,timestamp:1664470728
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728254
2022-09-29T22:28:48,254 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728254
2022-09-29T22:28:48,256 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,339 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:81.69|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b8b01e03-9004-429d-8db7-45b501415695,timestamp:1664470728
2022-09-29T22:28:48,341 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:81.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b8b01e03-9004-429d-8db7-45b501415695,timestamp:1664470728
2022-09-29T22:28:48,342 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:48,342 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:48,342 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 940
2022-09-29T22:28:48,342 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,342 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 851518640, Backend time ns: 87983442
2022-09-29T22:28:48,342 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 851518640, Backend time ns: 87983442
2022-09-29T22:28:48,342 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:851|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,343 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,343 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728343
2022-09-29T22:28:48,343 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728343
2022-09-29T22:28:48,343 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,428 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:48,428 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:48,428 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:84.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fcb514c5-1ffb-4cf2-9b94-0d82eddf5f11,timestamp:1664470728
2022-09-29T22:28:48,429 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 948
2022-09-29T22:28:48,429 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,429 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:84.22|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fcb514c5-1ffb-4cf2-9b94-0d82eddf5f11,timestamp:1664470728
2022-09-29T22:28:48,429 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 860946015, Backend time ns: 86666256
2022-09-29T22:28:48,429 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 860946015, Backend time ns: 86666256
2022-09-29T22:28:48,429 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:860|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,430 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,430 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728430
2022-09-29T22:28:48,430 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728430
2022-09-29T22:28:48,431 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 89
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:95.05|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:97bec5dc-ff9f-43b1-90c4-f35e42d33539,timestamp:1664470728
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 964
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:95.11|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:97bec5dc-ff9f-43b1-90c4-f35e42d33539,timestamp:1664470728
2022-09-29T22:28:48,527 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,527 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 865660490, Backend time ns: 97605886
2022-09-29T22:28:48,527 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 865660490, Backend time ns: 97605886
2022-09-29T22:28:48,528 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:865|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,528 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,528 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728528
2022-09-29T22:28:48,528 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728528
2022-09-29T22:28:48,529 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 88
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 88
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 972
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 882287679, Backend time ns: 88569309
2022-09-29T22:28:48,616 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 882287679, Backend time ns: 88569309
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:882|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728616
2022-09-29T22:28:48,616 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728616
2022-09-29T22:28:48,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:86.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8cf14379-737b-4211-9934-4e11844861f1,timestamp:1664470728
2022-09-29T22:28:48,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:86.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8cf14379-737b-4211-9934-4e11844861f1,timestamp:1664470728
2022-09-29T22:28:48,620 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.25|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7c78e4dd-a547-4c0f-b058-0e89b707e97c,timestamp:1664470728
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 980
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7c78e4dd-a547-4c0f-b058-0e89b707e97c,timestamp:1664470728
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,693 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 903121412, Backend time ns: 76544350
2022-09-29T22:28:48,693 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 903121412, Backend time ns: 76544350
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:903|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728693
2022-09-29T22:28:48,693 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728693
2022-09-29T22:28:48,694 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,796 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102
2022-09-29T22:28:48,796 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102
2022-09-29T22:28:48,797 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:101.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7c48257e-3e26-45d0-95ab-340c67f41f19,timestamp:1664470728
2022-09-29T22:28:48,797 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 982
2022-09-29T22:28:48,797 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,797 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:101.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7c48257e-3e26-45d0-95ab-340c67f41f19,timestamp:1664470728
2022-09-29T22:28:48,797 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 877745185, Backend time ns: 103862601
2022-09-29T22:28:48,797 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 877745185, Backend time ns: 103862601
2022-09-29T22:28:48,798 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:877|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,798 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,798 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728798
2022-09-29T22:28:48,798 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728798
2022-09-29T22:28:48,801 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:80.94|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:43b30957-1bde-46a6-9d25-1c232112d8b5,timestamp:1664470728
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 963
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,883 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 877778276, Backend time ns: 84766256
2022-09-29T22:28:48,883 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 877778276, Backend time ns: 84766256
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:80.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:43b30957-1bde-46a6-9d25-1c232112d8b5,timestamp:1664470728
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:877|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728883
2022-09-29T22:28:48,883 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728883
2022-09-29T22:28:48,886 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:83.31|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d18208a1-d406-4076-8510-638b63b8b6db,timestamp:1664470728
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 939
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:48,970 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 851702272, Backend time ns: 87000285
2022-09-29T22:28:48,970 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 851702272, Backend time ns: 87000285
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:851|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,970 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470728
2022-09-29T22:28:48,971 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728971
2022-09-29T22:28:48,971 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470728971
2022-09-29T22:28:48,974 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:83.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d18208a1-d406-4076-8510-638b63b8b6db,timestamp:1664470728
2022-09-29T22:28:48,974 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470728
2022-09-29T22:28:49,056 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:82.02|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d4c8b791-3925-4d4e-9b45-cef53fe2ba29,timestamp:1664470729
2022-09-29T22:28:49,057 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:49,057 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:82.08|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d4c8b791-3925-4d4e-9b45-cef53fe2ba29,timestamp:1664470729
2022-09-29T22:28:49,057 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85
2022-09-29T22:28:49,057 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 913
2022-09-29T22:28:49,058 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,058 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 826205952, Backend time ns: 87293921
2022-09-29T22:28:49,058 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 826205952, Backend time ns: 87293921
2022-09-29T22:28:49,058 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:826|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,058 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,058 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729058
2022-09-29T22:28:49,058 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729058
2022-09-29T22:28:49,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 71
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.96|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:237a7308-e5e0-432c-95e1-4c62267db999,timestamp:1664470729
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 874
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:70.05|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:237a7308-e5e0-432c-95e1-4c62267db999,timestamp:1664470729
2022-09-29T22:28:49,130 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,130 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 801328482, Backend time ns: 72358503
2022-09-29T22:28:49,130 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 801328482, Backend time ns: 72358503
2022-09-29T22:28:49,131 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:801|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,131 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729131
2022-09-29T22:28:49,131 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729131
2022-09-29T22:28:49,132 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,211 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:49,211 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:49,211 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:79.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a5bd0872-9d90-4fb6-82a6-959930a32c3b,timestamp:1664470729
2022-09-29T22:28:49,211 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 868
2022-09-29T22:28:49,212 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,212 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 787064756, Backend time ns: 81114104
2022-09-29T22:28:49,212 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 787064756, Backend time ns: 81114104
2022-09-29T22:28:49,212 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:787|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,212 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,213 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729213
2022-09-29T22:28:49,213 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729213
2022-09-29T22:28:49,213 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:79.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a5bd0872-9d90-4fb6-82a6-959930a32c3b,timestamp:1664470729
2022-09-29T22:28:49,215 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,297 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:49,297 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:49,297 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:82.13|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f18651a5-f813-4e7c-b7af-e0b92243704d,timestamp:1664470729
2022-09-29T22:28:49,297 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 865
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:82.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:f18651a5-f813-4e7c-b7af-e0b92243704d,timestamp:1664470729
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,298 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 780730177, Backend time ns: 85158058
2022-09-29T22:28:49,298 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 780730177, Backend time ns: 85158058
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:780|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729298
2022-09-29T22:28:49,298 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729298
2022-09-29T22:28:49,300 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,383 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:49,383 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 856
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,384 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 769190682, Backend time ns: 85713206
2022-09-29T22:28:49,384 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 769190682, Backend time ns: 85713206
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:769|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729384
2022-09-29T22:28:49,384 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729384
2022-09-29T22:28:49,385 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:82.87|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b91e9f3c-5b34-4a8e-aff3-c1a44aa93fd3,timestamp:1664470729
2022-09-29T22:28:49,385 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:82.92|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b91e9f3c-5b34-4a8e-aff3-c1a44aa93fd3,timestamp:1664470729
2022-09-29T22:28:49,385 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 93
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 93
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:91.47|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d0057125-6fb8-4ef8-91ab-90ec77c5b060,timestamp:1664470729
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:91.51|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d0057125-6fb8-4ef8-91ab-90ec77c5b060,timestamp:1664470729
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 858
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,477 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 764751316, Backend time ns: 93180649
2022-09-29T22:28:49,477 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 764751316, Backend time ns: 93180649
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:764|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729477
2022-09-29T22:28:49,477 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729477
2022-09-29T22:28:49,480 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 73
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 73
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:71.23|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:31e7307a-2eed-4175-a69f-941e0b1f7ab1,timestamp:1664470729
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 857
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,551 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:71.28|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:31e7307a-2eed-4175-a69f-941e0b1f7ab1,timestamp:1664470729
2022-09-29T22:28:49,552 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 767790250, Backend time ns: 74207848
2022-09-29T22:28:49,552 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 767790250, Backend time ns: 74207848
2022-09-29T22:28:49,552 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:767|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,552 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,552 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729552
2022-09-29T22:28:49,552 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729552
2022-09-29T22:28:49,552 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:49,628 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:49,628 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 830
2022-09-29T22:28:49,629 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.12|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5831549d-a7f2-4c52-b0b4-11f21a3214eb,timestamp:1664470729
2022-09-29T22:28:49,629 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,629 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 753316453, Backend time ns: 77210193
2022-09-29T22:28:49,629 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 753316453, Backend time ns: 77210193
2022-09-29T22:28:49,629 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5831549d-a7f2-4c52-b0b4-11f21a3214eb,timestamp:1664470729
2022-09-29T22:28:49,629 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:753|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,629 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,630 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729629
2022-09-29T22:28:49,630 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729629
2022-09-29T22:28:49,638 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,730 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:49,730 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:49,730 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:91.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0218a07a-d6ab-4ca2-b859-ecc84aaf4a3d,timestamp:1664470729
2022-09-29T22:28:49,730 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 844
2022-09-29T22:28:49,730 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,731 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 743535575, Backend time ns: 101087674
2022-09-29T22:28:49,731 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 743535575, Backend time ns: 101087674
2022-09-29T22:28:49,731 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:91.84|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0218a07a-d6ab-4ca2-b859-ecc84aaf4a3d,timestamp:1664470729
2022-09-29T22:28:49,731 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:743|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,731 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,731 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729731
2022-09-29T22:28:49,731 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729731
2022-09-29T22:28:49,733 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,831 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 95
2022-09-29T22:28:49,831 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 95
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 860
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:97.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:65a44fa7-147b-4a15-a076-dfbee28207b6,timestamp:1664470729
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,832 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 757818993, Backend time ns: 101061272
2022-09-29T22:28:49,832 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 757818993, Backend time ns: 101061272
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:757|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729832
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729832
2022-09-29T22:28:49,832 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:97.06|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:65a44fa7-147b-4a15-a076-dfbee28207b6,timestamp:1664470729
2022-09-29T22:28:49,833 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:49,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 104
2022-09-29T22:28:49,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 104
2022-09-29T22:28:49,936 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:102.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:11768f8a-7339-4957-b4cb-32d1f06acef8,timestamp:1664470729
2022-09-29T22:28:49,937 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 877
2022-09-29T22:28:49,937 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:49,937 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:102.53|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:11768f8a-7339-4957-b4cb-32d1f06acef8,timestamp:1664470729
2022-09-29T22:28:49,937 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 772019158, Backend time ns: 105298632
2022-09-29T22:28:49,937 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 772019158, Backend time ns: 105298632
2022-09-29T22:28:49,938 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:772|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,938 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470729
2022-09-29T22:28:49,938 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729938
2022-09-29T22:28:49,938 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470729938
2022-09-29T22:28:49,939 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470729
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:66.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:98c14e6a-5d1f-48fb-a348-cf86dd031871,timestamp:1664470730
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 874
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:66.38|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:98c14e6a-5d1f-48fb-a348-cf86dd031871,timestamp:1664470730
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 806273551, Backend time ns: 68168416
2022-09-29T22:28:50,006 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 806273551, Backend time ns: 68168416
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:806|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730006
2022-09-29T22:28:50,006 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730006
2022-09-29T22:28:50,007 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:79.09|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:04942d24-710a-41f9-954a-94ca67e5b6f0,timestamp:1664470730
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 873
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 792086935, Backend time ns: 80818917
2022-09-29T22:28:50,087 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 792086935, Backend time ns: 80818917
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:792|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:79.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:04942d24-710a-41f9-954a-94ca67e5b6f0,timestamp:1664470730
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730087
2022-09-29T22:28:50,087 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730087
2022-09-29T22:28:50,090 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,169 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,169 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,170 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 871
2022-09-29T22:28:50,170 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,171 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:79.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:625471d4-0bed-4b67-9f27-c2ef601d1346,timestamp:1664470730
2022-09-29T22:28:50,172 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:79.54|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:625471d4-0bed-4b67-9f27-c2ef601d1346,timestamp:1664470730
2022-09-29T22:28:50,172 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 788129020, Backend time ns: 82405450
2022-09-29T22:28:50,172 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 788129020, Backend time ns: 82405450
2022-09-29T22:28:50,172 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:788|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,172 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,173 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730173
2022-09-29T22:28:50,173 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730173
2022-09-29T22:28:50,174 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,249 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:50,249 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 76
2022-09-29T22:28:50,249 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.28|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e2d00f3-d640-4cf7-81c7-87e7ae567b02,timestamp:1664470730
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 861
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,250 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 771860975, Backend time ns: 77191848
2022-09-29T22:28:50,250 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 771860975, Backend time ns: 77191848
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:771|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730250
2022-09-29T22:28:50,250 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730250
2022-09-29T22:28:50,252 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:5e2d00f3-d640-4cf7-81c7-87e7ae567b02,timestamp:1664470730
2022-09-29T22:28:50,253 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:78.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bf9bb150-8623-420a-a4c6-381cf719db2d,timestamp:1664470730
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 851
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,331 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 770094306, Backend time ns: 80730847
2022-09-29T22:28:50,331 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 770094306, Backend time ns: 80730847
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:78.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:bf9bb150-8623-420a-a4c6-381cf719db2d,timestamp:1664470730
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:770|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730331
2022-09-29T22:28:50,331 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730331
2022-09-29T22:28:50,334 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 863
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,415 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 778820666, Backend time ns: 83939444
2022-09-29T22:28:50,415 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 778820666, Backend time ns: 83939444
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:778|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730415
2022-09-29T22:28:50,415 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730415
2022-09-29T22:28:50,416 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:80.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:02f4f730-ad5c-4680-bc01-67c6173af8d8,timestamp:1664470730
2022-09-29T22:28:50,416 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:80.87|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:02f4f730-ad5c-4680-bc01-67c6173af8d8,timestamp:1664470730
2022-09-29T22:28:50,420 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 100
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:97.25|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6c7b71f7-ce95-43c4-8f1e-225bcfb471a1,timestamp:1664470730
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 879
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:97.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6c7b71f7-ce95-43c4-8f1e-225bcfb471a1,timestamp:1664470730
2022-09-29T22:28:50,518 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,519 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 776458326, Backend time ns: 103083099
2022-09-29T22:28:50,519 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 776458326, Backend time ns: 103083099
2022-09-29T22:28:50,519 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:776|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,519 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730519
2022-09-29T22:28:50,519 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730519
2022-09-29T22:28:50,521 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 107
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 107
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 894
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,626 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 786804025, Backend time ns: 107053890
2022-09-29T22:28:50,626 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 786804025, Backend time ns: 107053890
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:786|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:105.33|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7059eb9f-334c-48f8-af5b-8f7a2d87f9f5,timestamp:1664470730
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730626
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730626
2022-09-29T22:28:50,626 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:105.37|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:7059eb9f-334c-48f8-af5b-8f7a2d87f9f5,timestamp:1664470730
2022-09-29T22:28:50,628 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:78.86|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0512c8a2-63b1-4254-9d70-3e20786bf71e,timestamp:1664470730
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:78.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0512c8a2-63b1-4254-9d70-3e20786bf71e,timestamp:1664470730
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 874
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,707 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 792865400, Backend time ns: 81061129
2022-09-29T22:28:50,707 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 792865400, Backend time ns: 81061129
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:792|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730707
2022-09-29T22:28:50,707 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730707
2022-09-29T22:28:50,708 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 73
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 73
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.5|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a097f24-cee6-40e0-999b-923a921049b9,timestamp:1664470730
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 842
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,781 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 767757110, Backend time ns: 74053311
2022-09-29T22:28:50,781 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 767757110, Backend time ns: 74053311
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:767|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,781 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,782 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730782
2022-09-29T22:28:50,782 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730782
2022-09-29T22:28:50,782 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.55|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:8a097f24-cee6-40e0-999b-923a921049b9,timestamp:1664470730
2022-09-29T22:28:50,785 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 103
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 103
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:103.42|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d417b6ab-11ff-4c1e-ba2d-3ba1539f8ad9,timestamp:1664470730
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 882
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:103.46|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:d417b6ab-11ff-4c1e-ba2d-3ba1539f8ad9,timestamp:1664470730
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,889 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 774286363, Backend time ns: 107630342
2022-09-29T22:28:50,889 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 774286363, Backend time ns: 107630342
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:774|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730889
2022-09-29T22:28:50,889 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730889
2022-09-29T22:28:50,891 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:50,984 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 94
2022-09-29T22:28:50,984 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 94
2022-09-29T22:28:50,984 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 894
2022-09-29T22:28:50,984 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:50,985 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 798910880, Backend time ns: 95264088
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:92.17|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:25e47499-3c83-4ff8-910c-833f0d85d842,timestamp:1664470730
2022-09-29T22:28:50,985 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 798910880, Backend time ns: 95264088
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:798|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:92.27|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:25e47499-3c83-4ff8-910c-833f0d85d842,timestamp:1664470730
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470730
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730985
2022-09-29T22:28:50,985 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470730985
2022-09-29T22:28:50,986 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470730
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:236a1a28-e482-4c56-b381-f91ec6e80bcd,timestamp:1664470731
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 887
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:73.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:236a1a28-e482-4c56-b381-f91ec6e80bcd,timestamp:1664470731
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,060 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 811725738, Backend time ns: 75410866
2022-09-29T22:28:51,060 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 811725738, Backend time ns: 75410866
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:811|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731060
2022-09-29T22:28:51,060 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731060
2022-09-29T22:28:51,061 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,136 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,136 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,136 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b07abc17-c688-4467-b1d8-466646ea7778,timestamp:1664470731
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.21|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:b07abc17-c688-4467-b1d8-466646ea7778,timestamp:1664470731
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 885
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,137 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 808102650, Backend time ns: 76518343
2022-09-29T22:28:51,137 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 808102650, Backend time ns: 76518343
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:808|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731137
2022-09-29T22:28:51,137 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731137
2022-09-29T22:28:51,141 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:75.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:64ee8b73-0de3-47e1-8885-9d4b01cc9735,timestamp:1664470731
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 882
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:75.94|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:64ee8b73-0de3-47e1-8885-9d4b01cc9735,timestamp:1664470731
2022-09-29T22:28:51,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 804558376, Backend time ns: 77771679
2022-09-29T22:28:51,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 804558376, Backend time ns: 77771679
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:804|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731215
2022-09-29T22:28:51,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731215
2022-09-29T22:28:51,217 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:57.39|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:beb30205-23ae-42bf-b576-ee42c1817190,timestamp:1664470731
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 855
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:57.43|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:beb30205-23ae-42bf-b576-ee42c1817190,timestamp:1664470731
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,274 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 795591504, Backend time ns: 58879299
2022-09-29T22:28:51,274 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 795591504, Backend time ns: 58879299
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:795|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731274
2022-09-29T22:28:51,274 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731274
2022-09-29T22:28:51,276 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,329 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:51,329 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 54
2022-09-29T22:28:51,329 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.93|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ced9702a-68ae-4ae0-b0c8-499ba1243996,timestamp:1664470731
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.96|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ced9702a-68ae-4ae0-b0c8-499ba1243996,timestamp:1664470731
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 810
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,330 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 753958752, Backend time ns: 55391234
2022-09-29T22:28:51,330 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 753958752, Backend time ns: 55391234
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:753|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731330
2022-09-29T22:28:51,330 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731330
2022-09-29T22:28:51,331 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15c76c48-c1f6-49ef-ae65-ba2ba02b4fa2,timestamp:1664470731
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:15c76c48-c1f6-49ef-ae65-ba2ba02b4fa2,timestamp:1664470731
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 760
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,387 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 702859839, Backend time ns: 56924416
2022-09-29T22:28:51,387 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 702859839, Backend time ns: 56924416
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:702|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731387
2022-09-29T22:28:51,387 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731387
2022-09-29T22:28:51,388 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 84
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:82.67|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0d3c49f3-574d-4c39-bf99-b8c86cd7c5fb,timestamp:1664470731
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:82.72|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0d3c49f3-574d-4c39-bf99-b8c86cd7c5fb,timestamp:1664470731
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 763
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,471 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 678822292, Backend time ns: 84215819
2022-09-29T22:28:51,471 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 678822292, Backend time ns: 84215819
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:678|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731471
2022-09-29T22:28:51,471 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731471
2022-09-29T22:28:51,473 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 57
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:56.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:48a5e14f-5b5a-4158-8894-bdb0725ca377,timestamp:1664470731
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:56.39|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:48a5e14f-5b5a-4158-8894-bdb0725ca377,timestamp:1664470731
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 745
2022-09-29T22:28:51,529 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,529 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 686948206, Backend time ns: 58021085
2022-09-29T22:28:51,529 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 686948206, Backend time ns: 58021085
2022-09-29T22:28:51,530 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:686|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,530 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,530 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731530
2022-09-29T22:28:51,530 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731530
2022-09-29T22:28:51,532 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.4|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c5b592ed-a0ba-4d63-94bc-bed0a210543d,timestamp:1664470731
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 715
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.48|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c5b592ed-a0ba-4d63-94bc-bed0a210543d,timestamp:1664470731
2022-09-29T22:28:51,605 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 639221292, Backend time ns: 75548250
2022-09-29T22:28:51,605 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 639221292, Backend time ns: 75548250
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:639|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731605
2022-09-29T22:28:51,605 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731605
2022-09-29T22:28:51,606 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.1|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a3163d1d-73c4-4329-9d82-12e2c1d728ef,timestamp:1664470731
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.14|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a3163d1d-73c4-4329-9d82-12e2c1d728ef,timestamp:1664470731
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 679
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,666 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 618154415, Backend time ns: 60709716
2022-09-29T22:28:51,666 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 618154415, Backend time ns: 60709716
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:618|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731666
2022-09-29T22:28:51,666 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731666
2022-09-29T22:28:51,668 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 66
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:66.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0442ad0b-f248-44b7-99a2-4f36f6c8f98a,timestamp:1664470731
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 673
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:66.8|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0442ad0b-f248-44b7-99a2-4f36f6c8f98a,timestamp:1664470731
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,734 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 604193837, Backend time ns: 68168758
2022-09-29T22:28:51,734 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 604193837, Backend time ns: 68168758
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:604|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,734 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,735 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731735
2022-09-29T22:28:51,735 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731735
2022-09-29T22:28:51,735 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,809 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74
2022-09-29T22:28:51,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.73|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a5e19274-6795-41d5-ab6e-f20a5e7f4c4b,timestamp:1664470731
2022-09-29T22:28:51,809 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:73.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:a5e19274-6795-41d5-ab6e-f20a5e7f4c4b,timestamp:1664470731
2022-09-29T22:28:51,809 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 669
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,810 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 594165014, Backend time ns: 75090606
2022-09-29T22:28:51,810 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 594165014, Backend time ns: 75090606
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:594|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731810
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731810
2022-09-29T22:28:51,810 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:51,880 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:69.79|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:64f554cf-36cb-4ddb-8db6-16191b416086,timestamp:1664470731
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 665
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:69.82|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:64f554cf-36cb-4ddb-8db6-16191b416086,timestamp:1664470731
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,881 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 593118381, Backend time ns: 70981121
2022-09-29T22:28:51,881 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 593118381, Backend time ns: 70981121
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:593|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731881
2022-09-29T22:28:51,881 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731881
2022-09-29T22:28:51,882 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:53.66|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fef45159-beb9-4b50-8ae7-67f34c308956,timestamp:1664470731
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:53.69|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fef45159-beb9-4b50-8ae7-67f34c308956,timestamp:1664470731
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 661
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,936 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605578517, Backend time ns: 55097383
2022-09-29T22:28:51,936 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605578517, Backend time ns: 55097383
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:605|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731936
2022-09-29T22:28:51,936 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731936
2022-09-29T22:28:51,937 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:51,996 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:51,996 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:59.21|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2424c95c-e8e2-4261-a557-d76829c15178,timestamp:1664470731
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 666
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:59.26|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2424c95c-e8e2-4261-a557-d76829c15178,timestamp:1664470731
2022-09-29T22:28:51,997 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605124471, Backend time ns: 60577127
2022-09-29T22:28:51,997 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605124471, Backend time ns: 60577127
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:605|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470731
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731997
2022-09-29T22:28:51,997 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470731997
2022-09-29T22:28:51,998 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470731
2022-09-29T22:28:52,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:52,077 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 80
2022-09-29T22:28:52,077 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:79.32|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:29d0fb93-61d4-42b9-96f9-e03636b075d2,timestamp:1664470732
2022-09-29T22:28:52,077 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:79.36|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:29d0fb93-61d4-42b9-96f9-e03636b075d2,timestamp:1664470732
2022-09-29T22:28:52,077 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 689
2022-09-29T22:28:52,078 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 608898579, Backend time ns: 80720542
2022-09-29T22:28:52,078 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 608898579, Backend time ns: 80720542
2022-09-29T22:28:52,078 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:608|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,078 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,078 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732078
2022-09-29T22:28:52,078 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732078
2022-09-29T22:28:52,079 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 63
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:60.97|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ed2118d-e256-4b97-9d7a-8a93d9dff279,timestamp:1664470732
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 669
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:61.01|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0ed2118d-e256-4b97-9d7a-8a93d9dff279,timestamp:1664470732
2022-09-29T22:28:52,141 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605288919, Backend time ns: 63323233
2022-09-29T22:28:52,141 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 605288919, Backend time ns: 63323233
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:605|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732141
2022-09-29T22:28:52,141 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732141
2022-09-29T22:28:52,143 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,214 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:28:52,214 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 72
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:72.21|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4e55069-21fd-4b64-ae9f-79969b584ed9,timestamp:1664470732
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:72.25|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c4e55069-21fd-4b64-ae9f-79969b584ed9,timestamp:1664470732
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 684
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 610284971, Backend time ns: 73505363
2022-09-29T22:28:52,215 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 610284971, Backend time ns: 73505363
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:610|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732215
2022-09-29T22:28:52,215 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732215
2022-09-29T22:28:52,216 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:74.12|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0f886ad3-7740-4600-993c-250127e1a5f6,timestamp:1664470732
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:74.15|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:0f886ad3-7740-4600-993c-250127e1a5f6,timestamp:1664470732
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 684
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 608552224, Backend time ns: 75305645
2022-09-29T22:28:52,290 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 608552224, Backend time ns: 75305645
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:608|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732290
2022-09-29T22:28:52,290 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732290
2022-09-29T22:28:52,291 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,376 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:84.89|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4b9583e1-5eea-4763-96be-e70c1c0a43af,timestamp:1664470732
2022-09-29T22:28:52,377 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:84.93|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:4b9583e1-5eea-4763-96be-e70c1c0a43af,timestamp:1664470732
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 87
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46750 "POST /predictions/benchmark HTTP/1.0" 200 711
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,378 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 623182353, Backend time ns: 87803797
2022-09-29T22:28:52,378 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 623182353, Backend time ns: 87803797
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:623|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732378
2022-09-29T22:28:52,378 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732378
2022-09-29T22:28:52,384 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,496 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117
2022-09-29T22:28:52,496 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117
2022-09-29T22:28:52,496 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46678 "POST /predictions/benchmark HTTP/1.0" 200 760
2022-09-29T22:28:52,496 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,497 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 642579635, Backend time ns: 118208578
2022-09-29T22:28:52,497 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 642579635, Backend time ns: 118208578
2022-09-29T22:28:52,497 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:642|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,497 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,497 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732497
2022-09-29T22:28:52,497 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732497
2022-09-29T22:28:52,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:111.29|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1f96fa7b-3660-4b9a-ad3a-aca474d7a85e,timestamp:1664470732
2022-09-29T22:28:52,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:111.44|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:1f96fa7b-3660-4b9a-ad3a-aca474d7a85e,timestamp:1664470732
2022-09-29T22:28:52,498 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:80.7|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6f3c73e4-7e31-4f9f-9e76-83378ef387ed,timestamp:1664470732
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:80.74|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:6f3c73e4-7e31-4f9f-9e76-83378ef387ed,timestamp:1664470732
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46684 "POST /predictions/benchmark HTTP/1.0" 200 770
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,580 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 686063886, Backend time ns: 83098334
2022-09-29T22:28:52,580 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 686063886, Backend time ns: 83098334
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:686|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732580
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732580
2022-09-29T22:28:52,580 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:65.81|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fbd3e7b5-ac26-47f3-a226-60923afb8628,timestamp:1664470732
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:65.85|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fbd3e7b5-ac26-47f3-a226-60923afb8628,timestamp:1664470732
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46708 "POST /predictions/benchmark HTTP/1.0" 200 765
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,647 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 698009900, Backend time ns: 66888746
2022-09-29T22:28:52,647 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 698009900, Backend time ns: 66888746
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:698|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732647
2022-09-29T22:28:52,647 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732647
2022-09-29T22:28:52,648 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 52
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:51.3|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35b5b9d3-eeb0-4cc2-bbc0-3306dcef6252,timestamp:1664470732
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46698 "POST /predictions/benchmark HTTP/1.0" 200 762
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:51.34|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:35b5b9d3-eeb0-4cc2-bbc0-3306dcef6252,timestamp:1664470732
2022-09-29T22:28:52,699 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,700 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 710100519, Backend time ns: 52521853
2022-09-29T22:28:52,700 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 710100519, Backend time ns: 52521853
2022-09-29T22:28:52,700 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:710|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,700 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,700 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732700
2022-09-29T22:28:52,700 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732700
2022-09-29T22:28:52,702 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,777 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:52,777 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 77
2022-09-29T22:28:52,777 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:76.41|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c45fa1a3-9dee-4683-83ea-b6b666ca9d8f,timestamp:1664470732
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46712 "POST /predictions/benchmark HTTP/1.0" 200 780
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:76.45|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c45fa1a3-9dee-4683-83ea-b6b666ca9d8f,timestamp:1664470732
2022-09-29T22:28:52,778 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 701733205, Backend time ns: 78180600
2022-09-29T22:28:52,778 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 701733205, Backend time ns: 78180600
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:701|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732778
2022-09-29T22:28:52,778 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732778
2022-09-29T22:28:52,780 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 75
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:73.88|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ff3697e6-c7fe-4e4f-bf46-5555642280b5,timestamp:1664470732
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:73.91|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:ff3697e6-c7fe-4e4f-bf46-5555642280b5,timestamp:1664470732
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46728 "POST /predictions/benchmark HTTP/1.0" 200 775
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,854 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 699303553, Backend time ns: 75885353
2022-09-29T22:28:52,854 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 699303553, Backend time ns: 75885353
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:699|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732854
2022-09-29T22:28:52,854 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732854
2022-09-29T22:28:52,855 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 55
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:54.72|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b90d4d5-2fc0-4ac2-8748-8968a4a6b01b,timestamp:1664470732
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:54.76|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:2b90d4d5-2fc0-4ac2-8748-8968a4a6b01b,timestamp:1664470732
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46748 "POST /predictions/benchmark HTTP/1.0" 200 768
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,910 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 711955401, Backend time ns: 56134698
2022-09-29T22:28:52,910 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 711955401, Backend time ns: 56134698
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:711|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,910 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,911 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732911
2022-09-29T22:28:52,911 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732911
2022-09-29T22:28:52,911 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 56
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:55.58|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c8de55fe-65c0-47cc-b45e-e2d19bbc188b,timestamp:1664470732
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46734 "POST /predictions/benchmark HTTP/1.0" 200 751
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:55.62|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:c8de55fe-65c0-47cc-b45e-e2d19bbc188b,timestamp:1664470732
2022-09-29T22:28:52,967 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 694674210, Backend time ns: 56775536
2022-09-29T22:28:52,967 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 694674210, Backend time ns: 56775536
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:694|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:0|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470732
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732967
2022-09-29T22:28:52,967 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1664470732967
2022-09-29T22:28:52,968 [INFO ] W-9000-benchmark_1.0-stdout MODEL_LOG - Backend received inference at: 1664470732
2022-09-29T22:28:53,032 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:28:53,032 [INFO ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 64
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:64.08|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb3a4bc0-92a8-425b-876e-533413e4f5e6,timestamp:1664470733
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:64.12|#ModelName:benchmark,Level:Model|#hostname:sasikumar-ThinkPad-T470,requestID:fb3a4bc0-92a8-425b-876e-533413e4f5e6,timestamp:1664470733
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0 ACCESS_LOG - /127.0.0.1:46752 "POST /predictions/benchmark HTTP/1.0" 200 742
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
2022-09-29T22:28:53,033 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 676313983, Backend time ns: 65332760
2022-09-29T22:28:53,033 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.job.Job - Waiting time ns: 676313983, Backend time ns: 65332760
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0 TS_METRICS - QueueTime.ms:676|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470733
2022-09-29T22:28:53,033 [INFO ] W-9000-benchmark_1.0 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470733
2022-09-29T22:28:53,037 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:28:53,037 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelVersionedRefs - Removed model: benchmark version: 1.0
2022-09-29T22:28:53,038 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:28:53,038 [DEBUG] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_MODEL_LOADED -> WORKER_SCALED_DOWN
2022-09-29T22:28:53,039 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,039 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,039 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,039 [WARN ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,039 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:28:53,039 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_SCALED_DOWN
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_SCALED_DOWN
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Shutting down the thread .. Scaling down.
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-benchmark_1.0 State change WORKER_SCALED_DOWN -> WORKER_STOPPED
2022-09-29T22:28:53,042 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,042 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,042 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,042 [WARN ] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:28:53,042 [DEBUG] W-9000-benchmark_1.0 org.pytorch.serve.wlm.WorkerThread - Worker terminated due to scale-down call.
2022-09-29T22:28:53,044 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,044 [INFO ] W-9000-benchmark_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stdout
2022-09-29T22:28:53,056 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:28:53,056 [INFO ] epollEventLoopGroup-3-23 org.pytorch.serve.wlm.ModelManager - Model benchmark unregistered.
2022-09-29T22:28:53,059 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,059 [INFO ] W-9000-benchmark_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-benchmark_1.0-stderr
2022-09-29T22:28:53,065 [INFO ] epollEventLoopGroup-3-23 ACCESS_LOG - /127.0.0.1:43196 "DELETE /models/benchmark HTTP/1.1" 200 28
2022-09-29T22:28:53,065 [INFO ] epollEventLoopGroup-3-23 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:sasikumar-ThinkPad-T470,timestamp:1664470712
